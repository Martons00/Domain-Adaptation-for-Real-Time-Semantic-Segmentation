{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyZZHWGhBryZ",
        "outputId": "073e7d6b-a0e7-4392-d17b-6bd953b1a3d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-01-16 09:49:26--  https://zenodo.org/records/5706578/files/Train.zip?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.43.25, 188.185.48.194, 188.185.45.92, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.43.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4021669263 (3.7G) [application/octet-stream]\n",
            "Saving to: ‘/content/Train.zip’\n",
            "\n",
            "/content/Train.zip  100%[===================>]   3.75G  12.0MB/s    in 5m 26s  \n",
            "\n",
            "2025-01-16 09:54:53 (11.8 MB/s) - ‘/content/Train.zip’ saved [4021669263/4021669263]\n",
            "\n",
            "Extracted files: ['Train']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "# URL for the dataset\n",
        "url = \"https://zenodo.org/records/5706578/files/Train.zip?download=1\"\n",
        "\n",
        "# Download the file using wget\n",
        "!wget -O /content/Train.zip \"$url\"\n",
        "\n",
        "# Define the extraction path\n",
        "extract_path = '/content/datasets/Train/'\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Extract the ZIP file\n",
        "with zipfile.ZipFile('/content/Train.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# List the contents of the extracted folder\n",
        "extracted_files = os.listdir(extract_path)\n",
        "print(\"Extracted files:\", extracted_files)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZrVX3hiBtzy",
        "outputId": "37c9a034-b799-43fa-aa3e-65467a10ae48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-01-16 09:55:30--  https://zenodo.org/records/5706578/files/Val.zip?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.48.194, 188.185.45.92, 188.185.43.25, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.48.194|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2425958254 (2.3G) [application/octet-stream]\n",
            "Saving to: ‘/content/Val.zip’\n",
            "\n",
            "/content/Val.zip    100%[===================>]   2.26G  4.80MB/s    in 7m 17s  \n",
            "\n",
            "2025-01-16 10:02:48 (5.29 MB/s) - ‘/content/Val.zip’ saved [2425958254/2425958254]\n",
            "\n",
            "Extracted files: ['Val']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "# URL for the dataset\n",
        "url = \"https://zenodo.org/records/5706578/files/Val.zip?download=1\"\n",
        "\n",
        "# Download the file using wget\n",
        "!wget -O /content/Val.zip \"$url\"\n",
        "\n",
        "# Define the extraction path\n",
        "extract_path = '/content/datasets/Val/'\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Extract the ZIP file\n",
        "with zipfile.ZipFile('/content/Val.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# List the contents of the extracted folder\n",
        "extracted_files = os.listdir(extract_path)\n",
        "print(\"Extracted files:\", extracted_files)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uUIp5h6JBv5t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import OrderedDict\n",
        "from torchvision import models\n",
        "\n",
        "expansion = 4\n",
        "\n",
        "class ConvBN(nn.Module):  # Convolutional followed by Batch Norm\n",
        "    def __init__(self, in_planes, out_planes, kernel_size=1, stride=1, padding=0, dilation=1):\n",
        "        super(ConvBN, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
        "                              padding=padding, dilation=dilation, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(out_planes, eps=1e-5, momentum=1e-3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.bn(self.conv(x))\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, stride=1, dilation=1, downsample=False, dropout_rate=0.5):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        mid_planes = out_planes // expansion\n",
        "        self.conv1 = ConvBN(in_planes, mid_planes, kernel_size=1, stride=stride)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Dropout after the first convolution\n",
        "        self.dropout1 = nn.Dropout2d(p=dropout_rate)\n",
        "\n",
        "        self.conv2 = ConvBN(mid_planes, mid_planes, kernel_size=3, stride=1, padding=dilation, dilation=dilation)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Dropout after the second convolution\n",
        "        self.dropout2 = nn.Dropout2d(p=dropout_rate)\n",
        "\n",
        "        self.conv3 = ConvBN(mid_planes, out_planes, kernel_size=1)\n",
        "        self.relu3 = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Dropout after the third convolution\n",
        "        self.dropout3 = nn.Dropout2d(p=dropout_rate)\n",
        "\n",
        "        if downsample:\n",
        "            self.shortcut = ConvBN(in_planes, out_planes, kernel_size=1, stride=stride)\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = self.shortcut(x)\n",
        "        out = self.relu1(self.conv1(x))\n",
        "\n",
        "        # Apply dropout after the first convolution\n",
        "        out = self.dropout1(out)\n",
        "\n",
        "        out = self.relu2(self.conv2(out))\n",
        "\n",
        "        # Apply dropout after the second convolution\n",
        "        out = self.dropout2(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "\n",
        "        # Apply dropout after the third convolution\n",
        "        out = self.dropout3(out)\n",
        "\n",
        "        out += identity\n",
        "        return self.relu3(out)\n",
        "\n",
        "def make_layer(blocks, in_planes, out_planes, stride, dilation, dropout_rate=0.5):\n",
        "    layers = OrderedDict()\n",
        "    layers['block1'] = Bottleneck(in_planes, out_planes, stride=stride, dilation=dilation, downsample=True, dropout_rate=dropout_rate)\n",
        "    for i in range(1, blocks):\n",
        "        layers[f'block{i+1}'] = Bottleneck(out_planes, out_planes, stride=1, dilation=dilation, dropout_rate=dropout_rate)\n",
        "    return nn.Sequential(layers)\n",
        "\n",
        "class ASPP(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, atrous_rates):\n",
        "        super(ASPP, self).__init__()\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=1,\n",
        "                      padding=rate, dilation=rate, bias=True) for rate in atrous_rates\n",
        "        ])\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for conv in self.convs:\n",
        "            nn.init.normal_(conv.weight, mean=0, std=0.01)\n",
        "            nn.init.constant_(conv.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return sum(conv(x) for conv in self.convs)\n",
        "\n",
        "\n",
        "# Define DeepLabV2 Model\n",
        "class DeepLabV2(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(DeepLabV2, self).__init__()\n",
        "\n",
        "        from torchvision.models import ResNet101_Weights\n",
        "        model = models.resnet101(weights=ResNet101_Weights.IMAGENET1K_V1)\n",
        "\n",
        "\n",
        "        # Keep only layers up to layer4\n",
        "        self.backbone = nn.Sequential(*(list(model.children())[:-2]))  # Exclude the final FC layer\n",
        "        self.aspp = nn.ModuleList([\n",
        "            nn.Conv2d(2048, 256, kernel_size=3, padding=r, dilation=r, bias=True) # list of modules with different dilation rates\n",
        "            for r in [6, 12, 18, 24]\n",
        "        ])\n",
        "        self.classifier = nn.Conv2d(256, n_classes, kernel_size=1)\n",
        "\n",
        "        # Add upsampling layer\n",
        "        #self.upsample = nn.Upsample(scale_factor=32, mode='bilinear', align_corners=True) # Upsample by 32 to match input size\n",
        "        self.upsample = nn.Upsample(size=(720, 720), mode='bilinear', align_corners=True)  # Match target size\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        aspp_out = sum(aspp(x) for aspp in self.aspp) # the outputs of the four convolutions are summed together\n",
        "\n",
        "        x = self.classifier(aspp_out) # Apply the classifier\n",
        "\n",
        "        # Upsample the output\n",
        "        x = self.upsample(x) # Apply upsampling\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3yGuYJ3yCd3i"
      },
      "outputs": [],
      "source": [
        "class OhemCrossEntropy(nn.Module):\n",
        "    def __init__(self, ignore_label=-1, thres=0.7,\n",
        "                 min_kept=100000, weight=None):\n",
        "        super(OhemCrossEntropy, self).__init__()\n",
        "        self.thresh = thres\n",
        "        self.min_kept = max(1, min_kept)\n",
        "        self.ignore_label = ignore_label\n",
        "        self.criterion = nn.CrossEntropyLoss(\n",
        "            weight=weight,\n",
        "            ignore_index=ignore_label,\n",
        "            reduction='none'\n",
        "        )\n",
        "\n",
        "    def _ce_forward(self, score, target):\n",
        "\n",
        "\n",
        "        loss = self.criterion(score, target)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def _ohem_forward(self, score, target, **kwargs):\n",
        "\n",
        "        pred = F.softmax(score, dim=1)\n",
        "        pixel_losses = self.criterion(score, target).contiguous().view(-1)\n",
        "        mask = target.contiguous().view(-1) != self.ignore_label\n",
        "        tmp_target = target.clone()\n",
        "        tmp_target[tmp_target == self.ignore_label] = 0\n",
        "        pred = pred.gather(1, tmp_target.unsqueeze(1))\n",
        "        pred, ind = pred.contiguous().view(-1,)[mask].contiguous().sort()\n",
        "        min_value = pred[min(self.min_kept, pred.numel() - 1)]\n",
        "        threshold = max(min_value, self.thresh)\n",
        "\n",
        "        pixel_losses = pixel_losses[mask][ind]\n",
        "        pixel_losses = pixel_losses[pred < threshold]\n",
        "        return pixel_losses.mean()\n",
        "\n",
        "    def forward(self, score, target):\n",
        "\n",
        "        if not (isinstance(score, list) or isinstance(score, tuple)):\n",
        "            score = [score]\n",
        "\n",
        "        #params from adam_oce_scheduler configuration\n",
        "\n",
        "        balance_weights = [0.4, 1.0]\n",
        "        sb_weights = 0.5\n",
        "        if len(balance_weights) == len(score):\n",
        "            functions = [self._ce_forward] * \\\n",
        "                (len(balance_weights) - 1) + [self._ohem_forward]\n",
        "            return sum([\n",
        "                w * func(x, target)\n",
        "                for (w, x, func) in zip(balance_weights, score, functions)\n",
        "            ])\n",
        "\n",
        "        elif len(score) == 1:\n",
        "            return sb_weights * self._ohem_forward(score[0], target)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"lengths of prediction and target are not identical!\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2, ignore_label=0, weight=None):\n",
        "\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.ignore_label = ignore_label\n",
        "        self.criterion = nn.CrossEntropyLoss(weight=weight, ignore_index=ignore_label, reduction='none')\n",
        "\n",
        "    def forward(self, score, target):\n",
        "\n",
        "        # Compute standard cross-entropy loss\n",
        "        ce_loss = self.criterion(score, target)\n",
        "\n",
        "        # Compute probability of the target class\n",
        "        probs = F.softmax(score, dim=1)\n",
        "        target_probs = probs.gather(1, target.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "        # Apply the focal loss formula\n",
        "        focal_weight = self.alpha * (1 - target_probs) ** self.gamma\n",
        "        focal_loss = focal_weight * ce_loss\n",
        "\n",
        "        # Return the average loss\n",
        "        return focal_loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4M9zGbJHBxv1",
        "outputId": "eff82e66-888e-4966-9338-5ddce89bc0c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: thop in /usr/local/lib/python3.11/dist-packages (0.1.1.post2209072238)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from thop) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->thop) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->thop) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->thop) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->thop) (3.0.2)\n",
            "batch_size = 6\n",
            "lr = 0.001\n",
            "Optimizer: Adam\n",
            "Criterion: FocalLoss\n",
            "Scheduler: CosineAnnealingLR\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20: 100%|██████████| 228/228 [04:38<00:00,  1.22s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20] - Loss: 1.3027\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 166/166 [02:11<00:00,  1.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation - Loss: 1.0573, IoU: 0.1858, Latency: 0.032477 sec\n",
            "FLOPs: 1.10e+12, Params: 6.14e+07\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20:  17%|█▋        | 38/228 [00:47<03:54,  1.24s/it]"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "!pip install thop\n",
        "from thop import profile\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "# Replace with your dataset class and helper functions\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms, models\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "class SimpleSegmentationDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, preferred_resolution=(720, 720), original_resolution=(1024, 1024), transform=None, augment=False, validation=False):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.preferred_resolution = preferred_resolution\n",
        "        self.original_resolution = original_resolution\n",
        "        self.transform = transform\n",
        "        self.augment = augment\n",
        "        self.validation = validation\n",
        "        self.images = sorted(os.listdir(image_dir))\n",
        "        self.masks = sorted(os.listdir(mask_dir))\n",
        "\n",
        "        # Training augmentation transforms\n",
        "        self.aug_transform = A.Compose([\n",
        "            A.Resize(height=self.preferred_resolution[0], width=self.preferred_resolution[1], p=1.0),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.VerticalFlip(p=0.5),\n",
        "            A.Rotate(limit=30, p=0.5),\n",
        "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
        "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "\n",
        "\n",
        "        # Validation transform (original resolution)\n",
        "        self.val_transform = A.Compose([\n",
        "            A.Resize(height=self.original_resolution[0], width=self.original_resolution[1], p=1.0),\n",
        "            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "            ToTensorV2(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(os.path.join(self.image_dir, self.images[idx])).convert('RGB')\n",
        "        mask = Image.open(os.path.join(self.mask_dir, self.masks[idx]))\n",
        "\n",
        "        # Convert image and mask to numpy arrays\n",
        "        image = np.array(image)\n",
        "        mask = np.array(mask)\n",
        "\n",
        "        if self.validation:\n",
        "            # Apply validation transform for original resolution\n",
        "            transformed = self.val_transform(image=image, mask=mask)\n",
        "        else:\n",
        "            # Apply augmentation transform for training\n",
        "            transformed = self.aug_transform(image=image, mask=mask)\n",
        "\n",
        "        image = transformed[\"image\"]\n",
        "        mask = transformed[\"mask\"]\n",
        "\n",
        "        # Ensure mask is a LongTensor for CrossEntropyLoss\n",
        "        mask = mask.clone().detach().to(dtype=torch.long)\n",
        "\n",
        "        return image, mask\n",
        "\n",
        "\n",
        "# Define Transform for Validation\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "# Dice Loss Implementation\n",
        "class DiceLossIgnoringIndex0(nn.Module):\n",
        "    def __init__(self, eps=1e-6):\n",
        "        super(DiceLossIgnoringIndex0, self).__init__()\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, preds, targets):\n",
        "        if preds.shape[1] > 1:\n",
        "            preds = F.softmax(preds, dim=1)\n",
        "        num_classes = preds.shape[1]\n",
        "        targets_one_hot = F.one_hot(targets, num_classes=num_classes).permute(0, 3, 1, 2).float()\n",
        "        preds = preds[:, 1:]\n",
        "        targets_one_hot = targets_one_hot[:, 1:]\n",
        "        intersection = torch.sum(preds * targets_one_hot, dim=(2, 3))\n",
        "        union = torch.sum(preds, dim=(2, 3)) + torch.sum(targets_one_hot, dim=(2, 3))\n",
        "        dice_score = (2.0 * intersection + self.eps) / (union + self.eps)\n",
        "        loss = 1.0 - dice_score.mean()\n",
        "        return loss\n",
        "\n",
        "\n",
        "\n",
        "# Calculate IoU (Intersection over Union) for validation\n",
        "def calculate_iou(output, target, num_classes):\n",
        "    output = torch.argmax(output, dim=1)\n",
        "    iou_list = []\n",
        "    for i in range(num_classes):\n",
        "        intersection = ((output == i) & (target == i)).sum().float()\n",
        "        union = ((output == i) | (target == i)).sum().float()\n",
        "        iou = intersection / (union + 1e-6)  # Avoid division by zero\n",
        "        iou_list.append(iou.item())\n",
        "    return np.mean(iou_list)\n",
        "\n",
        "\n",
        "def calculate_iou_ignore_index_0(output, target, num_classes):\n",
        "    \"\"\"\n",
        "    Calculate mean IoU (mIoU) for each class, ignoring class index 0.\n",
        "\n",
        "    Args:\n",
        "    - output (Tensor): The predicted output (batch_size, height, width)\n",
        "    - target (Tensor): The ground truth target mask (batch_size, height, width)\n",
        "    - num_classes (int): The number of classes in the segmentation task\n",
        "\n",
        "    Returns:\n",
        "    - (float): The mean IoU over all classes, excluding class index 0.\n",
        "    \"\"\"\n",
        "    output = torch.argmax(output, dim=1)\n",
        "    iou_list = []\n",
        "\n",
        "    for i in range(1, num_classes):  # Start from 1 to ignore index 0\n",
        "        intersection = ((output == i) & (target == i)).sum().float()\n",
        "        union = ((output == i) | (target == i)).sum().float()\n",
        "        iou = intersection / (union + 1e-6)  # Avoid division by zero\n",
        "        iou_list.append(iou.item())\n",
        "\n",
        "    return np.mean(iou_list) if iou_list else 0.0\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_predictions(images, predictions, ground_truths, num_classes):\n",
        "    for idx, (image, pred, gt) in enumerate(zip(images, predictions, ground_truths)):\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        plt.subplot(1, 3, 1)\n",
        "        plt.title(\"Image\")\n",
        "        plt.imshow(image.permute(1, 2, 0).cpu().numpy())  # Original input image\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.title(\"Prediction\")\n",
        "        plt.imshow(pred.cpu().numpy(), cmap='tab20', vmin=0, vmax=num_classes-1)  # Prediction mask\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.title(\"Ground Truth\")\n",
        "        plt.imshow(gt.cpu().numpy(), cmap='tab20', vmin=0, vmax=num_classes-1)  # Actual ground truth mask\n",
        "\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "\n",
        "def train(preferred_resolution=(512, 512)):\n",
        "    # Paths and Hyperparameters\n",
        "    dataset_dir = \"datasets/Train/Train/Rural\"\n",
        "    output_dir = \"checkpoints\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    log_dir = \"logs\"\n",
        "    batch_size = 6\n",
        "    num_classes = 8\n",
        "    lr = 0.001\n",
        "    epochs = 20\n",
        "    save_interval = 5\n",
        "\n",
        "    print(f\"batch_size = {batch_size}\")\n",
        "    print(f\"lr = {lr}\")\n",
        "\n",
        "\n",
        "    # Dataset Paths\n",
        "    train_images = os.path.join(dataset_dir, \"images_png\")\n",
        "    train_masks = os.path.join(dataset_dir, \"masks_png\")\n",
        "    val_dir = \"datasets/Val/Val/Rural\"\n",
        "    val_images = os.path.join(val_dir, \"images_png\")\n",
        "    val_masks = os.path.join(val_dir, \"masks_png\")\n",
        "\n",
        "    # Datasets and DataLoaders\n",
        "    train_dataset = SimpleSegmentationDataset(train_images, train_masks, preferred_resolution=preferred_resolution, augment=True)\n",
        "    val_dataset = SimpleSegmentationDataset(val_images, val_masks, original_resolution=(1024, 1024), augment=False, validation=True)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    # Model Initialization\n",
        "    model = DeepLabV2(n_classes=num_classes)\n",
        "    model = nn.DataParallel(model).cuda()\n",
        "\n",
        "    # Optimizer and Loss\n",
        "    #optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-4)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
        "\n",
        "    #criterion = nn.CrossEntropyLoss()\n",
        "    #criterion = DiceLossIgnoringIndex0()\n",
        "    #criterion = OhemCrossEntropy(ignore_label=0)\n",
        "    criterion = FocalLoss()\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
        "\n",
        "    print(f\"Optimizer: {type(optimizer).__name__}\")  # Print the optimizer name\n",
        "    print(f\"Criterion: {type(criterion).__name__}\")  # Print the name of the criterion class\n",
        "    print(f\"Scheduler: {type(scheduler).__name__}\") # Print the name of the scheduler class\n",
        "\n",
        "    # TensorBoard Setup\n",
        "    writer = SummaryWriter(log_dir=log_dir)\n",
        "\n",
        "    # Profiling FLOPs and Parameters (only once before training starts)\n",
        "    images, _ = next(iter(val_loader))  # Get a single batch\n",
        "    images = images.cuda()  # Move images to GPU\n",
        "\n",
        "    # Temporarily move the model to CPU for profiling\n",
        "    model_cpu = model.module.cpu()  # Extract the model from DataParallel\n",
        "    flops, params = profile(model_cpu, inputs=(images.cpu(),), verbose=False)  # Use images on CPU\n",
        "\n",
        "    model = model.cuda()  # Move model back to GPU\n",
        "\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
        "            images, masks = images.cuda(), masks.cuda()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        writer.add_scalar(\"Loss/train\", avg_loss, epoch)\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_iou = 0.0\n",
        "        total_latency = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, masks in tqdm(val_loader, desc=\"Validation\"):\n",
        "                images, masks = images.cuda(), masks.cuda()\n",
        "\n",
        "                # Measure latency\n",
        "                start_time = time.time()\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(images)  # Outputs are at wanted resolution\n",
        "\n",
        "                end_time = time.time()\n",
        "                total_latency += (end_time - start_time)\n",
        "\n",
        "\n",
        "                # Upsample the outputs to match the original mask resolution\n",
        "                outputs_upsampled = torch.nn.functional.interpolate(outputs, size=(1024, 1024), mode='bilinear', align_corners=False)\n",
        "\n",
        "                # Compute Loss\n",
        "                loss = criterion(outputs_upsampled, masks)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Compute IoU\n",
        "                #val_iou += calculate_iou(outputs_upsampled, masks, num_classes)\n",
        "                val_iou += calculate_iou_ignore_index_0(outputs_upsampled, masks, num_classes)\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        avg_val_iou = val_iou / len(val_loader)\n",
        "        avg_latency = total_latency / len(val_loader)\n",
        "        avg_latency_per_image = avg_latency / batch_size\n",
        "\n",
        "\n",
        "        # Logging Metrics\n",
        "        writer.add_scalar(\"Loss/val\", avg_val_loss, epoch)\n",
        "        writer.add_scalar(\"IoU/val\", avg_val_iou, epoch)\n",
        "        writer.add_scalar(\"Latency/val\", avg_latency, epoch)\n",
        "        writer.add_scalar(\"FLOPs\", flops, epoch)\n",
        "        writer.add_scalar(\"Parameters\", params, epoch)\n",
        "\n",
        "        print(f\"Validation - Loss: {avg_val_loss:.4f}, IoU: {avg_val_iou:.4f}, Latency: {avg_latency_per_image:.6f} sec\")\n",
        "        print(f\"FLOPs: {flops:.2e}, Params: {params:.2e}\")\n",
        "\n",
        "\n",
        "        # Step Scheduler\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        # Save Model Checkpoint\n",
        "        if (epoch + 1) % save_interval == 0:\n",
        "            checkpoint_path = os.path.join(output_dir, f\"model_epoch_{epoch+1}.pth\")\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': avg_loss,\n",
        "            }, checkpoint_path)\n",
        "\n",
        "    writer.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example: Train with a preferred resolution of 512x512\n",
        "    train(preferred_resolution=(720, 720))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
