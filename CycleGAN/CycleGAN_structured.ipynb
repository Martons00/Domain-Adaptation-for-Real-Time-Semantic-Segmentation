{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93MU7BiOj2dM"
      },
      "source": [
        "Train.zip import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-yuwbOHzutC",
        "outputId": "376ab09a-040a-4a28-f01a-19baa4e7f931"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-01-23 13:57:14--  https://zenodo.org/records/5706578/files/Train.zip?download=1\n",
            "Resolving zenodo.org (zenodo.org)... 188.185.45.92, 188.185.43.25, 188.185.48.194, ...\n",
            "Connecting to zenodo.org (zenodo.org)|188.185.45.92|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4021669263 (3.7G) [application/octet-stream]\n",
            "Saving to: ‘/content/Train.zip’\n",
            "\n",
            "/content/Train.zip  100%[===================>]   3.75G  5.70MB/s    in 9m 3s   \n",
            "\n",
            "2025-01-23 14:06:18 (7.06 MB/s) - ‘/content/Train.zip’ saved [4021669263/4021669263]\n",
            "\n",
            "Extracted files: ['Train']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "# URL for the dataset\n",
        "url = \"https://zenodo.org/records/5706578/files/Train.zip?download=1\"\n",
        "\n",
        "# Download the file using wget\n",
        "!wget -O /content/Train.zip \"$url\"\n",
        "\n",
        "# Define the extraction path\n",
        "extract_path = 'Train/'\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# Extract the ZIP file\n",
        "with zipfile.ZipFile('./Train.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "# List the contents of the extracted folder\n",
        "extracted_files = os.listdir(extract_path)\n",
        "print(\"Extracted files:\", extracted_files)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Laegt7vOwU_p",
        "outputId": "61f32809-3c8b-4a29-a1d2-73f0a9af94ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'pytorch-CycleGAN-and-pix2pix'...\n",
            "remote: Enumerating objects: 2516, done.\u001b[K\n",
            "remote: Total 2516 (delta 0), reused 0 (delta 0), pack-reused 2516 (from 1)\u001b[K\n",
            "Receiving objects: 100% (2516/2516), 8.20 MiB | 20.94 MiB/s, done.\n",
            "Resolving deltas: 100% (1575/1575), done.\n",
            "/content/pytorch-CycleGAN-and-pix2pix\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (0.20.1+cu121)\n",
            "Collecting dominate>=2.4.0 (from -r requirements.txt (line 3))\n",
            "  Downloading dominate-2.9.1-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Collecting visdom>=0.1.8.8 (from -r requirements.txt (line 4))\n",
            "  Downloading visdom-0.2.4.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (0.19.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.4.0->-r requirements.txt (line 1)) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->-r requirements.txt (line 1)) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.4.0->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision>=0.5.0->-r requirements.txt (line 2)) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision>=0.5.0->-r requirements.txt (line 2)) (11.1.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (2.32.3)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.11/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (6.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.17.0)\n",
            "Requirement already satisfied: jsonpatch in /usr/local/lib/python3.11/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.33)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from visdom>=0.1.8.8->-r requirements.txt (line 4)) (1.8.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (2.10.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (6.0.2)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (75.1.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 5)) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb->-r requirements.txt (line 5)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb->-r requirements.txt (line 5)) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->visdom>=0.1.8.8->-r requirements.txt (line 4)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->visdom>=0.1.8.8->-r requirements.txt (line 4)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->visdom>=0.1.8.8->-r requirements.txt (line 4)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->visdom>=0.1.8.8->-r requirements.txt (line 4)) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.4.0->-r requirements.txt (line 1)) (3.0.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch->visdom>=0.1.8.8->-r requirements.txt (line 4)) (3.0.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 5)) (5.0.2)\n",
            "Downloading dominate-2.9.1-py2.py3-none-any.whl (29 kB)\n",
            "Building wheels for collected packages: visdom\n",
            "  Building wheel for visdom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for visdom: filename=visdom-0.2.4-py3-none-any.whl size=1408196 sha256=14a1b797a9cb8524cd279a3b4fb51d197b5cc467ceb3fff7b28eee271ae6cd2d\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/a4/bb/2be445c295d88a74f9c0a4232f04860ca489a5c7c57eb959d9\n",
            "Successfully built visdom\n",
            "Installing collected packages: dominate, visdom\n",
            "Successfully installed dominate-2.9.1 visdom-0.2.4\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix.git\n",
        "%cd pytorch-CycleGAN-and-pix2pix\n",
        "!pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9DWRuUW47hY",
        "outputId": "b002f766-8a95-4838-f251-8c5bd64a500c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "print(os.path.isdir('/content/Train'))  # Should return True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wb66_xHAKdao"
      },
      "outputs": [],
      "source": [
        "##\n",
        "# for training, the dataroot must be:\n",
        "# directory_inserted_as_dataroot\n",
        "# │\n",
        "# ├── trainA\n",
        "# │   └── images_png      (source images)\n",
        "# │\n",
        "# └── trainB\n",
        "#    └── images_png      (target images)\n",
        "\n",
        "#\n",
        "# Be careful to not have any anything else int the directory.\n",
        "#\n",
        "\n",
        "# For the test:\n",
        "# directory_inserted_as_dataroot\n",
        "# │\n",
        "# └── testA\n",
        "#    └── images_png      (source images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MoLK430YzaIT",
        "outputId": "cdb5ccf4-48ef-49ea-c8e7-3c7c96a6b1d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------- Options ---------------\n",
            "               batch_size: 1                             \n",
            "                    beta1: 0.5                           \n",
            "          checkpoints_dir: ./checkpoints                 \n",
            "           continue_train: True                          \t[default: False]\n",
            "                crop_size: 256                           \n",
            "                 dataroot: /content/Train/Train/Rural    \t[default: None]\n",
            "             dataset_mode: unaligned                     \n",
            "                direction: AtoB                          \n",
            "              display_env: main                          \n",
            "             display_freq: 400                           \n",
            "               display_id: 0                             \t[default: 1]\n",
            "            display_ncols: 4                             \n",
            "             display_port: 8097                          \n",
            "           display_server: http://localhost              \n",
            "          display_winsize: 256                           \n",
            "                    epoch: latest                        \n",
            "              epoch_count: 20                            \t[default: 1]\n",
            "                 gan_mode: lsgan                         \n",
            "                  gpu_ids: 0                             \n",
            "                init_gain: 0.02                          \n",
            "                init_type: normal                        \n",
            "                 input_nc: 3                             \n",
            "                  isTrain: True                          \t[default: None]\n",
            "                 lambda_A: 10.0                          \n",
            "                 lambda_B: 10.0                          \n",
            "          lambda_identity: 0.5                           \n",
            "                load_iter: 0                             \t[default: 0]\n",
            "                load_size: 286                           \n",
            "                       lr: 0.0002                        \n",
            "           lr_decay_iters: 50                            \n",
            "                lr_policy: linear                        \n",
            "         max_dataset_size: inf                           \n",
            "                    model: cycle_gan                     \n",
            "                 n_epochs: 20                            \t[default: 100]\n",
            "           n_epochs_decay: 10                            \t[default: 100]\n",
            "               n_layers_D: 3                             \n",
            "                     name: prova                         \t[default: experiment_name]\n",
            "                      ndf: 64                            \n",
            "                     netD: basic                         \n",
            "                     netG: resnet_9blocks                \n",
            "                      ngf: 64                            \n",
            "               no_dropout: True                          \n",
            "                  no_flip: False                         \n",
            "                  no_html: False                         \n",
            "                     norm: instance                      \n",
            "              num_threads: 4                             \n",
            "                output_nc: 3                             \n",
            "                    phase: train                         \n",
            "                pool_size: 50                            \n",
            "               preprocess: resize_and_crop               \n",
            "               print_freq: 100                           \n",
            "             save_by_iter: False                         \n",
            "          save_epoch_freq: 3                             \t[default: 5]\n",
            "         save_latest_freq: 5000                          \n",
            "           serial_batches: False                         \n",
            "                   suffix:                               \n",
            "         update_html_freq: 1000                          \n",
            "                use_wandb: False                         \n",
            "                  verbose: False                         \n",
            "       wandb_project_name: CycleGAN-and-pix2pix          \n",
            "----------------- End -------------------\n",
            "dataset [UnalignedDataset] was created\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "The number of training images = 1366\n",
            "initialize network with normal\n",
            "initialize network with normal\n",
            "initialize network with normal\n",
            "initialize network with normal\n",
            "model [CycleGANModel] was created\n",
            "loading the model from ./checkpoints/prova/latest_net_G_A.pth\n",
            "/content/pytorch-CycleGAN-and-pix2pix/models/base_model.py:192: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(load_path, map_location=str(self.device))\n",
            "loading the model from ./checkpoints/prova/latest_net_G_B.pth\n",
            "loading the model from ./checkpoints/prova/latest_net_D_A.pth\n",
            "loading the model from ./checkpoints/prova/latest_net_D_B.pth\n",
            "---------- Networks initialized -------------\n",
            "[Network G_A] Total number of parameters : 11.378 M\n",
            "[Network G_B] Total number of parameters : 11.378 M\n",
            "[Network D_A] Total number of parameters : 2.765 M\n",
            "[Network D_B] Total number of parameters : 2.765 M\n",
            "-----------------------------------------------\n",
            "create web directory ./checkpoints/prova/web...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:224: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n",
            "learning rate 0.0002000 -> 0.0001818\n",
            "(epoch: 20, iters: 100, time: 0.568, data: 0.446) D_A: 0.283 G_A: 0.290 cycle_A: 0.965 idt_A: 0.530 D_B: 0.215 G_B: 0.279 cycle_B: 0.688 idt_B: 0.408 \n",
            "(epoch: 20, iters: 200, time: 0.593, data: 0.002) D_A: 0.138 G_A: 0.455 cycle_A: 0.749 idt_A: 0.260 D_B: 0.157 G_B: 0.423 cycle_B: 0.608 idt_B: 0.321 \n",
            "(epoch: 20, iters: 300, time: 0.621, data: 0.002) D_A: 0.205 G_A: 0.259 cycle_A: 0.686 idt_A: 0.419 D_B: 0.398 G_B: 0.028 cycle_B: 0.897 idt_B: 0.345 \n",
            "(epoch: 20, iters: 400, time: 0.892, data: 0.002) D_A: 0.138 G_A: 0.796 cycle_A: 0.410 idt_A: 0.538 D_B: 0.171 G_B: 0.304 cycle_B: 1.204 idt_B: 0.186 \n",
            "(epoch: 20, iters: 500, time: 0.613, data: 0.002) D_A: 0.204 G_A: 0.213 cycle_A: 0.732 idt_A: 0.300 D_B: 0.145 G_B: 0.556 cycle_B: 0.599 idt_B: 0.308 \n",
            "(epoch: 20, iters: 600, time: 0.617, data: 0.002) D_A: 0.299 G_A: 0.294 cycle_A: 0.600 idt_A: 0.262 D_B: 0.205 G_B: 0.768 cycle_B: 0.581 idt_B: 0.238 \n",
            "(epoch: 20, iters: 700, time: 0.611, data: 0.002) D_A: 0.229 G_A: 0.460 cycle_A: 1.459 idt_A: 0.499 D_B: 0.167 G_B: 0.343 cycle_B: 1.394 idt_B: 0.670 \n",
            "(epoch: 20, iters: 800, time: 0.611, data: 0.002) D_A: 0.193 G_A: 0.488 cycle_A: 0.652 idt_A: 0.246 D_B: 0.121 G_B: 0.360 cycle_B: 0.585 idt_B: 0.250 \n",
            "(epoch: 20, iters: 900, time: 0.610, data: 0.002) D_A: 0.239 G_A: 0.634 cycle_A: 0.766 idt_A: 0.244 D_B: 0.112 G_B: 0.326 cycle_B: 0.508 idt_B: 0.477 \n",
            "(epoch: 20, iters: 1000, time: 0.612, data: 0.002) D_A: 0.296 G_A: 0.490 cycle_A: 0.731 idt_A: 0.421 D_B: 0.063 G_B: 0.349 cycle_B: 0.792 idt_B: 0.307 \n",
            "(epoch: 20, iters: 1100, time: 0.612, data: 0.002) D_A: 0.236 G_A: 0.132 cycle_A: 1.232 idt_A: 0.393 D_B: 0.433 G_B: 0.325 cycle_B: 0.979 idt_B: 0.404 \n",
            "(epoch: 20, iters: 1200, time: 0.611, data: 0.002) D_A: 0.283 G_A: 0.446 cycle_A: 0.651 idt_A: 0.301 D_B: 0.064 G_B: 0.327 cycle_B: 0.648 idt_B: 0.231 \n",
            "(epoch: 20, iters: 1300, time: 0.613, data: 0.002) D_A: 0.359 G_A: 0.842 cycle_A: 0.548 idt_A: 0.243 D_B: 0.210 G_B: 0.094 cycle_B: 0.571 idt_B: 0.221 \n",
            "End of epoch 20 / 30 \t Time Taken: 667 sec\n",
            "learning rate 0.0001818 -> 0.0001636\n",
            "(epoch: 21, iters: 34, time: 0.615, data: 0.002) D_A: 0.123 G_A: 0.390 cycle_A: 0.850 idt_A: 0.264 D_B: 0.184 G_B: 0.634 cycle_B: 0.543 idt_B: 0.354 \n",
            "(epoch: 21, iters: 134, time: 0.613, data: 0.002) D_A: 0.336 G_A: 0.260 cycle_A: 0.561 idt_A: 0.471 D_B: 0.198 G_B: 0.258 cycle_B: 1.015 idt_B: 0.219 \n",
            "(epoch: 21, iters: 234, time: 1.044, data: 0.002) D_A: 0.237 G_A: 0.188 cycle_A: 0.920 idt_A: 0.404 D_B: 0.286 G_B: 0.399 cycle_B: 0.911 idt_B: 0.326 \n",
            "(epoch: 21, iters: 334, time: 0.616, data: 0.002) D_A: 0.257 G_A: 0.168 cycle_A: 1.269 idt_A: 0.250 D_B: 0.277 G_B: 0.563 cycle_B: 0.522 idt_B: 0.310 \n",
            "(epoch: 21, iters: 434, time: 0.601, data: 0.002) D_A: 0.201 G_A: 0.417 cycle_A: 0.843 idt_A: 0.467 D_B: 0.227 G_B: 0.110 cycle_B: 0.961 idt_B: 0.323 \n",
            "(epoch: 21, iters: 534, time: 0.613, data: 0.002) D_A: 0.138 G_A: 0.271 cycle_A: 0.848 idt_A: 0.231 D_B: 0.206 G_B: 0.192 cycle_B: 0.528 idt_B: 0.434 \n",
            "(epoch: 21, iters: 634, time: 0.856, data: 0.003) D_A: 0.173 G_A: 0.505 cycle_A: 0.315 idt_A: 0.298 D_B: 0.195 G_B: 0.377 cycle_B: 0.537 idt_B: 0.118 \n",
            "(epoch: 21, iters: 734, time: 0.613, data: 0.002) D_A: 0.117 G_A: 0.233 cycle_A: 0.601 idt_A: 0.368 D_B: 0.285 G_B: 0.294 cycle_B: 0.736 idt_B: 0.270 \n",
            "(epoch: 21, iters: 834, time: 0.614, data: 0.002) D_A: 0.192 G_A: 0.374 cycle_A: 0.471 idt_A: 0.222 D_B: 0.180 G_B: 0.456 cycle_B: 0.482 idt_B: 0.267 \n",
            "(epoch: 21, iters: 934, time: 0.616, data: 0.002) D_A: 0.246 G_A: 0.527 cycle_A: 0.491 idt_A: 0.370 D_B: 0.269 G_B: 0.238 cycle_B: 0.789 idt_B: 0.222 \n",
            "(epoch: 21, iters: 1034, time: 0.612, data: 0.013) D_A: 0.228 G_A: 0.443 cycle_A: 0.464 idt_A: 0.379 D_B: 0.278 G_B: 0.616 cycle_B: 0.626 idt_B: 0.183 \n",
            "(epoch: 21, iters: 1134, time: 0.614, data: 0.002) D_A: 0.121 G_A: 0.362 cycle_A: 0.536 idt_A: 0.517 D_B: 0.240 G_B: 0.276 cycle_B: 1.041 idt_B: 0.199 \n",
            "(epoch: 21, iters: 1234, time: 0.613, data: 0.002) D_A: 0.089 G_A: 0.564 cycle_A: 0.783 idt_A: 0.263 D_B: 0.066 G_B: 0.523 cycle_B: 0.533 idt_B: 0.378 \n",
            "(epoch: 21, iters: 1334, time: 0.612, data: 0.002) D_A: 0.208 G_A: 0.370 cycle_A: 0.515 idt_A: 0.266 D_B: 0.295 G_B: 0.358 cycle_B: 0.625 idt_B: 0.242 \n",
            "saving the model at the end of epoch 21, iters 2732\n",
            "End of epoch 21 / 30 \t Time Taken: 672 sec\n",
            "learning rate 0.0001636 -> 0.0001455\n",
            "(epoch: 22, iters: 68, time: 0.877, data: 0.002) D_A: 0.242 G_A: 0.345 cycle_A: 0.773 idt_A: 0.309 D_B: 0.228 G_B: 0.458 cycle_B: 0.716 idt_B: 0.334 \n",
            "(epoch: 22, iters: 168, time: 0.608, data: 0.002) D_A: 0.069 G_A: 0.462 cycle_A: 0.683 idt_A: 0.520 D_B: 0.206 G_B: 0.290 cycle_B: 1.066 idt_B: 0.301 \n",
            "(epoch: 22, iters: 268, time: 0.610, data: 0.009) D_A: 0.121 G_A: 0.204 cycle_A: 0.513 idt_A: 0.392 D_B: 0.185 G_B: 0.439 cycle_B: 0.794 idt_B: 0.271 \n",
            "(epoch: 22, iters: 368, time: 0.612, data: 0.014) D_A: 0.212 G_A: 0.288 cycle_A: 0.737 idt_A: 0.223 D_B: 0.334 G_B: 0.532 cycle_B: 0.466 idt_B: 0.310 \n",
            "(epoch: 22, iters: 468, time: 0.606, data: 0.002) D_A: 0.324 G_A: 0.161 cycle_A: 0.555 idt_A: 0.400 D_B: 0.317 G_B: 0.219 cycle_B: 0.807 idt_B: 0.247 \n",
            "(epoch: 22, iters: 568, time: 0.612, data: 0.002) D_A: 0.182 G_A: 0.063 cycle_A: 0.997 idt_A: 0.282 D_B: 0.163 G_B: 0.296 cycle_B: 0.784 idt_B: 0.414 \n",
            "(epoch: 22, iters: 668, time: 0.617, data: 0.002) D_A: 0.301 G_A: 0.252 cycle_A: 0.514 idt_A: 0.349 D_B: 0.157 G_B: 0.057 cycle_B: 0.670 idt_B: 0.264 \n",
            "(epoch: 22, iters: 768, time: 0.613, data: 0.013) D_A: 0.284 G_A: 0.282 cycle_A: 0.805 idt_A: 0.970 D_B: 0.222 G_B: 0.669 cycle_B: 1.876 idt_B: 0.368 \n",
            "(epoch: 22, iters: 868, time: 0.612, data: 0.002) D_A: 0.383 G_A: 0.166 cycle_A: 1.043 idt_A: 0.354 D_B: 0.239 G_B: 0.420 cycle_B: 0.559 idt_B: 0.436 \n",
            "(epoch: 22, iters: 968, time: 0.602, data: 0.002) D_A: 0.766 G_A: 1.066 cycle_A: 0.428 idt_A: 0.240 D_B: 0.347 G_B: 0.405 cycle_B: 0.500 idt_B: 0.174 \n",
            "(epoch: 22, iters: 1068, time: 0.610, data: 0.002) D_A: 0.096 G_A: 0.579 cycle_A: 0.656 idt_A: 0.339 D_B: 0.173 G_B: 0.248 cycle_B: 0.687 idt_B: 0.242 \n",
            "(epoch: 22, iters: 1168, time: 0.611, data: 0.015) D_A: 0.184 G_A: 0.328 cycle_A: 0.469 idt_A: 0.415 D_B: 0.379 G_B: 0.790 cycle_B: 0.953 idt_B: 0.243 \n",
            "(epoch: 22, iters: 1268, time: 0.878, data: 0.002) D_A: 0.370 G_A: 0.095 cycle_A: 0.697 idt_A: 0.391 D_B: 0.171 G_B: 0.181 cycle_B: 0.849 idt_B: 0.238 \n",
            "End of epoch 22 / 30 \t Time Taken: 670 sec\n",
            "learning rate 0.0001455 -> 0.0001273\n",
            "(epoch: 23, iters: 2, time: 0.617, data: 0.002) D_A: 0.139 G_A: 0.406 cycle_A: 0.899 idt_A: 0.265 D_B: 0.169 G_B: 0.224 cycle_B: 0.642 idt_B: 0.290 \n",
            "(epoch: 23, iters: 102, time: 0.602, data: 0.001) D_A: 0.265 G_A: 0.239 cycle_A: 0.705 idt_A: 0.223 D_B: 0.363 G_B: 0.160 cycle_B: 0.455 idt_B: 0.214 \n",
            "(epoch: 23, iters: 202, time: 0.610, data: 0.008) D_A: 0.168 G_A: 0.654 cycle_A: 0.631 idt_A: 0.280 D_B: 0.318 G_B: 0.156 cycle_B: 0.529 idt_B: 0.249 \n",
            "(epoch: 23, iters: 302, time: 0.869, data: 0.002) D_A: 0.274 G_A: 0.227 cycle_A: 0.889 idt_A: 0.260 D_B: 0.112 G_B: 0.190 cycle_B: 0.520 idt_B: 0.508 \n",
            "(epoch: 23, iters: 402, time: 0.611, data: 0.002) D_A: 0.030 G_A: 0.718 cycle_A: 0.402 idt_A: 0.227 D_B: 0.105 G_B: 0.416 cycle_B: 0.462 idt_B: 0.172 \n",
            "(epoch: 23, iters: 502, time: 0.615, data: 0.002) D_A: 0.063 G_A: 0.602 cycle_A: 0.617 idt_A: 0.235 D_B: 0.170 G_B: 0.573 cycle_B: 0.563 idt_B: 0.219 \n",
            "(epoch: 23, iters: 602, time: 0.613, data: 0.002) D_A: 0.282 G_A: 0.360 cycle_A: 0.532 idt_A: 0.210 D_B: 0.229 G_B: 0.262 cycle_B: 0.441 idt_B: 0.271 \n",
            "(epoch: 23, iters: 702, time: 0.612, data: 0.002) D_A: 0.254 G_A: 0.131 cycle_A: 0.486 idt_A: 0.554 D_B: 0.240 G_B: 0.305 cycle_B: 0.772 idt_B: 0.218 \n",
            "(epoch: 23, iters: 802, time: 0.611, data: 0.002) D_A: 0.210 G_A: 0.646 cycle_A: 0.589 idt_A: 0.280 D_B: 0.089 G_B: 0.389 cycle_B: 0.600 idt_B: 0.223 \n",
            "(epoch: 23, iters: 902, time: 0.613, data: 0.002) D_A: 0.352 G_A: 0.462 cycle_A: 0.499 idt_A: 0.279 D_B: 0.291 G_B: 0.185 cycle_B: 0.595 idt_B: 0.192 \n",
            "saving the latest model (epoch 23, total_iters 5000)\n",
            "(epoch: 23, iters: 1002, time: 0.613, data: 0.016) D_A: 0.203 G_A: 0.889 cycle_A: 0.978 idt_A: 0.324 D_B: 0.130 G_B: 0.516 cycle_B: 0.551 idt_B: 0.404 \n",
            "(epoch: 23, iters: 1102, time: 0.611, data: 0.002) D_A: 0.188 G_A: 0.419 cycle_A: 0.533 idt_A: 0.297 D_B: 0.205 G_B: 0.270 cycle_B: 0.683 idt_B: 0.262 \n",
            "(epoch: 23, iters: 1202, time: 0.613, data: 0.002) D_A: 0.085 G_A: 0.560 cycle_A: 0.437 idt_A: 0.495 D_B: 0.163 G_B: 0.825 cycle_B: 1.029 idt_B: 0.193 \n",
            "(epoch: 23, iters: 1302, time: 0.611, data: 0.002) D_A: 0.110 G_A: 0.578 cycle_A: 0.330 idt_A: 0.337 D_B: 0.093 G_B: 0.581 cycle_B: 0.623 idt_B: 0.162 \n",
            "End of epoch 23 / 30 \t Time Taken: 671 sec\n",
            "learning rate 0.0001273 -> 0.0001091\n",
            "(epoch: 24, iters: 36, time: 0.613, data: 0.013) D_A: 0.118 G_A: 0.623 cycle_A: 0.617 idt_A: 0.239 D_B: 0.230 G_B: 0.271 cycle_B: 0.544 idt_B: 0.233 \n",
            "(epoch: 24, iters: 136, time: 0.903, data: 0.002) D_A: 0.234 G_A: 0.376 cycle_A: 0.570 idt_A: 0.293 D_B: 0.193 G_B: 0.416 cycle_B: 0.593 idt_B: 0.238 \n",
            "(epoch: 24, iters: 236, time: 0.613, data: 0.002) D_A: 0.274 G_A: 0.185 cycle_A: 0.626 idt_A: 0.227 D_B: 0.235 G_B: 0.328 cycle_B: 0.476 idt_B: 0.332 \n",
            "(epoch: 24, iters: 336, time: 0.612, data: 0.002) D_A: 0.421 G_A: 0.550 cycle_A: 0.611 idt_A: 0.420 D_B: 0.131 G_B: 0.419 cycle_B: 0.850 idt_B: 0.214 \n",
            "(epoch: 24, iters: 436, time: 0.613, data: 0.008) D_A: 0.145 G_A: 0.515 cycle_A: 0.504 idt_A: 0.447 D_B: 0.098 G_B: 0.529 cycle_B: 0.817 idt_B: 0.223 \n",
            "(epoch: 24, iters: 536, time: 0.893, data: 0.002) D_A: 0.225 G_A: 0.160 cycle_A: 0.785 idt_A: 0.241 D_B: 0.086 G_B: 0.209 cycle_B: 0.656 idt_B: 0.234 \n",
            "(epoch: 24, iters: 636, time: 0.615, data: 0.002) D_A: 0.116 G_A: 0.210 cycle_A: 0.409 idt_A: 0.232 D_B: 0.149 G_B: 0.413 cycle_B: 0.565 idt_B: 0.255 \n",
            "(epoch: 24, iters: 736, time: 0.610, data: 0.002) D_A: 0.483 G_A: 0.162 cycle_A: 0.870 idt_A: 0.304 D_B: 0.188 G_B: 0.234 cycle_B: 0.585 idt_B: 0.427 \n",
            "(epoch: 24, iters: 836, time: 0.611, data: 0.011) D_A: 0.298 G_A: 0.504 cycle_A: 0.769 idt_A: 0.282 D_B: 0.292 G_B: 0.178 cycle_B: 0.663 idt_B: 0.254 \n",
            "(epoch: 24, iters: 936, time: 0.611, data: 0.002) D_A: 0.169 G_A: 0.293 cycle_A: 0.829 idt_A: 0.405 D_B: 0.148 G_B: 0.443 cycle_B: 0.674 idt_B: 0.363 \n",
            "(epoch: 24, iters: 1036, time: 0.609, data: 0.002) D_A: 0.092 G_A: 0.375 cycle_A: 2.001 idt_A: 0.255 D_B: 0.182 G_B: 0.187 cycle_B: 0.548 idt_B: 0.880 \n",
            "(epoch: 24, iters: 1136, time: 0.612, data: 0.002) D_A: 0.167 G_A: 0.107 cycle_A: 0.400 idt_A: 0.603 D_B: 0.098 G_B: 0.587 cycle_B: 1.195 idt_B: 0.213 \n",
            "(epoch: 24, iters: 1236, time: 0.615, data: 0.002) D_A: 0.114 G_A: 0.383 cycle_A: 0.584 idt_A: 0.312 D_B: 0.117 G_B: 0.394 cycle_B: 0.494 idt_B: 0.261 \n",
            "(epoch: 24, iters: 1336, time: 0.608, data: 0.002) D_A: 0.304 G_A: 0.403 cycle_A: 0.521 idt_A: 0.231 D_B: 0.138 G_B: 0.316 cycle_B: 0.523 idt_B: 0.288 \n",
            "saving the model at the end of epoch 24, iters 6830\n",
            "End of epoch 24 / 30 \t Time Taken: 671 sec\n",
            "learning rate 0.0001091 -> 0.0000909\n",
            "(epoch: 25, iters: 70, time: 0.611, data: 0.002) D_A: 0.333 G_A: 0.417 cycle_A: 0.760 idt_A: 0.334 D_B: 0.182 G_B: 0.220 cycle_B: 0.646 idt_B: 0.302 \n",
            "(epoch: 25, iters: 170, time: 0.610, data: 0.002) D_A: 0.294 G_A: 0.239 cycle_A: 0.576 idt_A: 0.337 D_B: 0.238 G_B: 0.300 cycle_B: 0.680 idt_B: 0.216 \n",
            "(epoch: 25, iters: 270, time: 0.615, data: 0.002) D_A: 0.198 G_A: 0.106 cycle_A: 0.404 idt_A: 0.260 D_B: 0.279 G_B: 0.501 cycle_B: 0.591 idt_B: 0.174 \n",
            "(epoch: 25, iters: 370, time: 0.900, data: 0.002) D_A: 0.205 G_A: 0.521 cycle_A: 0.825 idt_A: 0.246 D_B: 0.197 G_B: 0.208 cycle_B: 0.597 idt_B: 0.280 \n",
            "(epoch: 25, iters: 470, time: 0.611, data: 0.002) D_A: 0.258 G_A: 0.223 cycle_A: 1.245 idt_A: 0.359 D_B: 0.249 G_B: 0.228 cycle_B: 0.683 idt_B: 0.526 \n",
            "(epoch: 25, iters: 570, time: 0.612, data: 0.002) D_A: 0.150 G_A: 0.238 cycle_A: 0.632 idt_A: 0.210 D_B: 0.179 G_B: 0.520 cycle_B: 0.469 idt_B: 0.285 \n",
            "(epoch: 25, iters: 670, time: 0.614, data: 0.008) D_A: 0.112 G_A: 0.340 cycle_A: 0.632 idt_A: 0.215 D_B: 0.172 G_B: 0.251 cycle_B: 0.498 idt_B: 0.259 \n",
            "(epoch: 25, iters: 770, time: 0.612, data: 0.002) D_A: 0.282 G_A: 0.252 cycle_A: 1.417 idt_A: 0.266 D_B: 0.158 G_B: 0.490 cycle_B: 0.571 idt_B: 0.830 \n",
            "(epoch: 25, iters: 870, time: 0.611, data: 0.002) D_A: 0.224 G_A: 0.296 cycle_A: 0.651 idt_A: 0.373 D_B: 0.138 G_B: 0.374 cycle_B: 0.685 idt_B: 0.233 \n",
            "(epoch: 25, iters: 970, time: 0.613, data: 0.002) D_A: 0.129 G_A: 0.292 cycle_A: 0.503 idt_A: 0.235 D_B: 0.223 G_B: 0.303 cycle_B: 0.552 idt_B: 0.208 \n",
            "(epoch: 25, iters: 1070, time: 0.615, data: 0.002) D_A: 0.128 G_A: 0.323 cycle_A: 0.585 idt_A: 0.373 D_B: 0.160 G_B: 0.440 cycle_B: 0.779 idt_B: 0.216 \n",
            "(epoch: 25, iters: 1170, time: 0.884, data: 0.002) D_A: 0.168 G_A: 0.226 cycle_A: 0.819 idt_A: 0.327 D_B: 0.164 G_B: 0.517 cycle_B: 0.602 idt_B: 0.366 \n",
            "(epoch: 25, iters: 1270, time: 0.611, data: 0.002) D_A: 0.215 G_A: 0.278 cycle_A: 0.552 idt_A: 0.355 D_B: 0.147 G_B: 0.478 cycle_B: 0.695 idt_B: 0.206 \n",
            "End of epoch 25 / 30 \t Time Taken: 670 sec\n",
            "learning rate 0.0000909 -> 0.0000727\n",
            "(epoch: 26, iters: 4, time: 0.614, data: 0.002) D_A: 0.082 G_A: 0.213 cycle_A: 0.825 idt_A: 0.242 D_B: 0.320 G_B: 0.308 cycle_B: 0.512 idt_B: 0.346 \n",
            "(epoch: 26, iters: 104, time: 0.609, data: 0.002) D_A: 0.200 G_A: 0.242 cycle_A: 0.641 idt_A: 0.275 D_B: 0.100 G_B: 0.194 cycle_B: 0.513 idt_B: 0.297 \n",
            "(epoch: 26, iters: 204, time: 1.022, data: 0.002) D_A: 0.264 G_A: 0.255 cycle_A: 0.399 idt_A: 0.246 D_B: 0.217 G_B: 0.472 cycle_B: 0.510 idt_B: 0.218 \n",
            "(epoch: 26, iters: 304, time: 0.611, data: 0.002) D_A: 0.150 G_A: 0.558 cycle_A: 0.429 idt_A: 0.235 D_B: 0.036 G_B: 0.710 cycle_B: 0.461 idt_B: 0.229 \n",
            "(epoch: 26, iters: 404, time: 0.610, data: 0.002) D_A: 0.251 G_A: 0.378 cycle_A: 0.728 idt_A: 0.269 D_B: 0.178 G_B: 0.179 cycle_B: 0.484 idt_B: 0.233 \n",
            "(epoch: 26, iters: 504, time: 0.612, data: 0.002) D_A: 0.149 G_A: 0.443 cycle_A: 0.727 idt_A: 0.331 D_B: 0.163 G_B: 0.400 cycle_B: 0.637 idt_B: 0.346 \n",
            "(epoch: 26, iters: 604, time: 0.613, data: 0.002) D_A: 0.306 G_A: 0.104 cycle_A: 0.361 idt_A: 0.205 D_B: 0.396 G_B: 0.427 cycle_B: 0.474 idt_B: 0.188 \n",
            "(epoch: 26, iters: 704, time: 0.615, data: 0.002) D_A: 0.222 G_A: 0.223 cycle_A: 0.830 idt_A: 0.306 D_B: 0.170 G_B: 0.266 cycle_B: 0.761 idt_B: 0.257 \n",
            "(epoch: 26, iters: 804, time: 0.611, data: 0.010) D_A: 0.320 G_A: 0.178 cycle_A: 0.478 idt_A: 0.371 D_B: 0.187 G_B: 0.348 cycle_B: 0.671 idt_B: 0.298 \n",
            "(epoch: 26, iters: 904, time: 0.617, data: 0.002) D_A: 0.247 G_A: 0.212 cycle_A: 0.589 idt_A: 0.242 D_B: 0.450 G_B: 0.151 cycle_B: 0.690 idt_B: 0.277 \n",
            "(epoch: 26, iters: 1004, time: 0.609, data: 0.002) D_A: 0.345 G_A: 0.522 cycle_A: 1.040 idt_A: 0.233 D_B: 0.167 G_B: 0.264 cycle_B: 0.458 idt_B: 0.323 \n",
            "(epoch: 26, iters: 1104, time: 0.613, data: 0.002) D_A: 0.076 G_A: 0.750 cycle_A: 1.202 idt_A: 0.290 D_B: 0.100 G_B: 0.415 cycle_B: 0.737 idt_B: 0.568 \n",
            "(epoch: 26, iters: 1204, time: 0.615, data: 0.002) D_A: 0.397 G_A: 0.389 cycle_A: 0.506 idt_A: 0.248 D_B: 0.202 G_B: 0.242 cycle_B: 0.528 idt_B: 0.324 \n",
            "(epoch: 26, iters: 1304, time: 0.615, data: 0.008) D_A: 0.189 G_A: 1.031 cycle_A: 0.906 idt_A: 0.230 D_B: 0.120 G_B: 0.327 cycle_B: 0.567 idt_B: 0.431 \n",
            "End of epoch 26 / 30 \t Time Taken: 671 sec\n",
            "learning rate 0.0000727 -> 0.0000545\n",
            "(epoch: 27, iters: 38, time: 0.874, data: 0.002) D_A: 0.260 G_A: 0.260 cycle_A: 0.347 idt_A: 0.315 D_B: 0.212 G_B: 0.217 cycle_B: 0.742 idt_B: 0.153 \n",
            "(epoch: 27, iters: 138, time: 0.615, data: 0.002) D_A: 0.215 G_A: 0.510 cycle_A: 1.381 idt_A: 0.279 D_B: 0.074 G_B: 0.492 cycle_B: 0.534 idt_B: 0.776 \n",
            "(epoch: 27, iters: 238, time: 0.610, data: 0.002) D_A: 0.195 G_A: 0.230 cycle_A: 0.603 idt_A: 0.236 D_B: 0.272 G_B: 0.372 cycle_B: 0.498 idt_B: 0.243 \n",
            "(epoch: 27, iters: 338, time: 0.613, data: 0.002) D_A: 0.157 G_A: 0.362 cycle_A: 0.413 idt_A: 0.337 D_B: 0.159 G_B: 0.572 cycle_B: 0.896 idt_B: 0.257 \n",
            "(epoch: 27, iters: 438, time: 0.925, data: 0.002) D_A: 0.133 G_A: 0.231 cycle_A: 0.715 idt_A: 0.262 D_B: 0.205 G_B: 0.283 cycle_B: 0.567 idt_B: 0.315 \n",
            "saving the latest model (epoch 27, total_iters 10000)\n",
            "(epoch: 27, iters: 538, time: 0.614, data: 0.004) D_A: 0.272 G_A: 0.358 cycle_A: 0.516 idt_A: 0.333 D_B: 0.229 G_B: 0.307 cycle_B: 0.919 idt_B: 0.301 \n",
            "(epoch: 27, iters: 638, time: 0.612, data: 0.002) D_A: 0.136 G_A: 0.421 cycle_A: 1.305 idt_A: 0.341 D_B: 0.037 G_B: 0.801 cycle_B: 0.670 idt_B: 0.624 \n",
            "(epoch: 27, iters: 738, time: 0.611, data: 0.002) D_A: 0.055 G_A: 0.340 cycle_A: 0.591 idt_A: 0.663 D_B: 0.270 G_B: 0.418 cycle_B: 1.074 idt_B: 0.232 \n",
            "(epoch: 27, iters: 838, time: 0.610, data: 0.002) D_A: 0.223 G_A: 0.432 cycle_A: 0.426 idt_A: 0.353 D_B: 0.147 G_B: 0.359 cycle_B: 0.886 idt_B: 0.176 \n",
            "(epoch: 27, iters: 938, time: 0.618, data: 0.002) D_A: 0.246 G_A: 0.170 cycle_A: 0.844 idt_A: 0.217 D_B: 0.406 G_B: 0.387 cycle_B: 0.430 idt_B: 0.407 \n",
            "(epoch: 27, iters: 1038, time: 0.611, data: 0.010) D_A: 0.132 G_A: 0.286 cycle_A: 0.581 idt_A: 0.263 D_B: 0.102 G_B: 0.582 cycle_B: 0.552 idt_B: 0.218 \n",
            "(epoch: 27, iters: 1138, time: 0.610, data: 0.002) D_A: 0.112 G_A: 0.442 cycle_A: 0.493 idt_A: 0.259 D_B: 0.199 G_B: 0.508 cycle_B: 0.444 idt_B: 0.224 \n",
            "(epoch: 27, iters: 1238, time: 0.613, data: 0.002) D_A: 0.118 G_A: 0.433 cycle_A: 0.647 idt_A: 0.304 D_B: 0.154 G_B: 0.283 cycle_B: 0.489 idt_B: 0.327 \n",
            "(epoch: 27, iters: 1338, time: 0.611, data: 0.002) D_A: 0.339 G_A: 0.156 cycle_A: 0.442 idt_A: 0.222 D_B: 0.210 G_B: 0.444 cycle_B: 0.541 idt_B: 0.180 \n",
            "Traceback (most recent call last):\n",
            "  File \"/content/pytorch-CycleGAN-and-pix2pix/train.py\", line 52, in <module>\n",
            "    model.optimize_parameters()   # calculate loss functions, get gradients, update network weights\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/pytorch-CycleGAN-and-pix2pix/models/cycle_gan_model.py\", line 187, in optimize_parameters\n",
            "    self.backward_G()             # calculate gradients for G_A and G_B\n",
            "    ^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/pytorch-CycleGAN-and-pix2pix/models/cycle_gan_model.py\", line 178, in backward_G\n",
            "    self.loss_G.backward()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\", line 581, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n",
            "    _engine_run_backward(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n",
            "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!python train.py --dataroot '/content/Train/Train/Rural' --n_epochs 20 --n_epochs_decay 10 --name prova --model cycle_gan --display_id 0 --save_epoch_freq 3 --continue_train --epoch_count 20\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfyHXjmHC6M4",
        "outputId": "9d362abc-2ee8-4d0b-c63c-e7180f84c77c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/pytorch-CycleGAN-and-pix2pix/test.py\", line 30, in <module>\n",
            "    from options.test_options import TestOptions\n",
            "  File \"/content/pytorch-CycleGAN-and-pix2pix/options/test_options.py\", line 1, in <module>\n",
            "    from .base_options import BaseOptions\n",
            "  File \"/content/pytorch-CycleGAN-and-pix2pix/options/base_options.py\", line 3, in <module>\n",
            "    from util import util\n",
            "  File \"/content/pytorch-CycleGAN-and-pix2pix/util/util.py\", line 3, in <module>\n",
            "    import torch\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/__init__.py\", line 2059, in <module>\n",
            "    from torch import (\n",
            "  File \"<frozen importlib._bootstrap>\", line 1176, in _find_and_load\n",
            "  File \"<frozen importlib._bootstrap>\", line 1147, in _find_and_load_unlocked\n",
            "  File \"<frozen importlib._bootstrap>\", line 690, in _load_unlocked\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 936, in exec_module\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1032, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1131, in get_data\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!python test.py --dataroot '/content/Train/Train/Rural' --name prova --model test --no_dropout --num_test 1156 --preprocess none\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "psChs78NEXhV",
        "outputId": "5e488736-f41d-42c2-cf77-6fed79a547a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copied: 2387_fake.png\n",
            "Copied: 1698_fake.png\n",
            "Copied: 1500_fake.png\n",
            "Copied: 1425_fake.png\n",
            "Copied: 1371_fake.png\n",
            "Copied: 2314_fake.png\n",
            "Copied: 2447_fake.png\n",
            "Copied: 1539_fake.png\n",
            "Copied: 1972_fake.png\n",
            "Copied: 2241_fake.png\n",
            "Copied: 1758_fake.png\n",
            "Copied: 1836_fake.png\n",
            "Copied: 1830_fake.png\n",
            "Copied: 2292_fake.png\n",
            "Copied: 2109_fake.png\n",
            "Copied: 1787_fake.png\n",
            "Copied: 2139_fake.png\n",
            "Copied: 1711_fake.png\n",
            "Copied: 2162_fake.png\n",
            "Copied: 1616_fake.png\n",
            "Copied: 1639_fake.png\n",
            "Copied: 2315_fake.png\n",
            "Copied: 2058_fake.png\n",
            "Copied: 2302_fake.png\n",
            "Copied: 1589_fake.png\n",
            "Copied: 1482_fake.png\n",
            "Copied: 1377_fake.png\n",
            "Copied: 2253_fake.png\n",
            "Copied: 1990_fake.png\n",
            "Copied: 2056_fake.png\n",
            "Copied: 2344_fake.png\n",
            "Copied: 2079_fake.png\n",
            "Copied: 1467_fake.png\n",
            "Copied: 2142_fake.png\n",
            "Copied: 1973_fake.png\n",
            "Copied: 1960_fake.png\n",
            "Copied: 1625_fake.png\n",
            "Copied: 1472_fake.png\n",
            "Copied: 1961_fake.png\n",
            "Copied: 1387_fake.png\n",
            "Copied: 2499_fake.png\n",
            "Copied: 2115_fake.png\n",
            "Copied: 1995_fake.png\n",
            "Copied: 1508_fake.png\n",
            "Copied: 1637_fake.png\n",
            "Copied: 1513_fake.png\n",
            "Copied: 1367_fake.png\n",
            "Copied: 1524_fake.png\n",
            "Copied: 2320_fake.png\n",
            "Copied: 2332_fake.png\n",
            "Copied: 1918_fake.png\n",
            "Copied: 2236_fake.png\n",
            "Copied: 1478_fake.png\n",
            "Copied: 1820_fake.png\n",
            "Copied: 1540_fake.png\n",
            "Copied: 2227_fake.png\n",
            "Copied: 1709_fake.png\n",
            "Copied: 2354_fake.png\n",
            "Copied: 1806_fake.png\n",
            "Copied: 2347_fake.png\n",
            "Copied: 1487_fake.png\n",
            "Copied: 1379_fake.png\n",
            "Copied: 1561_fake.png\n",
            "Copied: 1865_fake.png\n",
            "Copied: 2319_fake.png\n",
            "Copied: 2036_fake.png\n",
            "Copied: 1828_fake.png\n",
            "Copied: 1663_fake.png\n",
            "Copied: 1578_fake.png\n",
            "Copied: 2124_fake.png\n",
            "Copied: 2059_fake.png\n",
            "Copied: 1493_fake.png\n",
            "Copied: 2435_fake.png\n",
            "Copied: 2405_fake.png\n",
            "Copied: 1786_fake.png\n",
            "Copied: 1477_fake.png\n",
            "Copied: 1402_fake.png\n",
            "Copied: 1976_fake.png\n",
            "Copied: 1892_fake.png\n",
            "Copied: 1382_fake.png\n",
            "Copied: 2258_fake.png\n",
            "Copied: 2508_fake.png\n",
            "Copied: 2093_fake.png\n",
            "Copied: 2475_fake.png\n",
            "Copied: 1445_fake.png\n",
            "Copied: 1785_fake.png\n",
            "Copied: 2448_fake.png\n",
            "Copied: 1668_fake.png\n",
            "Copied: 2291_fake.png\n",
            "Copied: 1706_fake.png\n",
            "Copied: 1862_fake.png\n",
            "Copied: 1929_fake.png\n",
            "Copied: 1564_fake.png\n",
            "Copied: 2086_fake.png\n",
            "Copied: 2037_fake.png\n",
            "Copied: 2338_fake.png\n",
            "Copied: 1931_fake.png\n",
            "Copied: 1763_fake.png\n",
            "Copied: 1565_fake.png\n",
            "Copied: 2001_fake.png\n",
            "Copied: 1628_fake.png\n",
            "Copied: 1871_fake.png\n",
            "Copied: 2465_fake.png\n",
            "Copied: 1443_fake.png\n",
            "Copied: 2271_fake.png\n",
            "Copied: 1958_fake.png\n",
            "Copied: 2491_fake.png\n",
            "Copied: 2119_fake.png\n",
            "Copied: 1533_fake.png\n",
            "Copied: 2516_fake.png\n",
            "Copied: 2006_fake.png\n",
            "Copied: 1802_fake.png\n",
            "Copied: 2098_fake.png\n",
            "Copied: 2396_fake.png\n",
            "Copied: 1949_fake.png\n",
            "Copied: 2210_fake.png\n",
            "Copied: 2002_fake.png\n",
            "Copied: 2272_fake.png\n",
            "Copied: 2261_fake.png\n",
            "Copied: 1571_fake.png\n",
            "Copied: 1531_fake.png\n",
            "Copied: 2419_fake.png\n",
            "Copied: 1707_fake.png\n",
            "Copied: 1393_fake.png\n",
            "Copied: 1609_fake.png\n",
            "Copied: 1463_fake.png\n",
            "Copied: 2477_fake.png\n",
            "Copied: 1773_fake.png\n",
            "Copied: 2431_fake.png\n",
            "Copied: 2025_fake.png\n",
            "Copied: 1420_fake.png\n",
            "Copied: 1400_fake.png\n",
            "Copied: 1715_fake.png\n",
            "Copied: 2141_fake.png\n",
            "Copied: 1939_fake.png\n",
            "Copied: 1940_fake.png\n",
            "Copied: 2421_fake.png\n",
            "Copied: 2351_fake.png\n",
            "Copied: 2026_fake.png\n",
            "Copied: 1951_fake.png\n",
            "Copied: 2377_fake.png\n",
            "Copied: 1414_fake.png\n",
            "Copied: 2382_fake.png\n",
            "Copied: 1372_fake.png\n",
            "Copied: 2057_fake.png\n",
            "Copied: 2376_fake.png\n",
            "Copied: 1415_fake.png\n",
            "Copied: 1657_fake.png\n",
            "Copied: 1430_fake.png\n",
            "Copied: 1744_fake.png\n",
            "Copied: 1799_fake.png\n",
            "Copied: 2287_fake.png\n",
            "Copied: 2088_fake.png\n",
            "Copied: 1490_fake.png\n",
            "Copied: 2138_fake.png\n",
            "Copied: 2416_fake.png\n",
            "Copied: 2246_fake.png\n",
            "Copied: 2486_fake.png\n",
            "Copied: 1392_fake.png\n",
            "Copied: 1868_fake.png\n",
            "Copied: 1831_fake.png\n",
            "Copied: 1390_fake.png\n",
            "Copied: 2336_fake.png\n",
            "Copied: 1650_fake.png\n",
            "Copied: 1704_fake.png\n",
            "Copied: 2374_fake.png\n",
            "Copied: 2229_fake.png\n",
            "Copied: 1437_fake.png\n",
            "Copied: 2063_fake.png\n",
            "Copied: 1808_fake.png\n",
            "Copied: 1876_fake.png\n",
            "Copied: 2359_fake.png\n",
            "Copied: 2346_fake.png\n",
            "Copied: 1741_fake.png\n",
            "Copied: 2050_fake.png\n",
            "Copied: 1840_fake.png\n",
            "Copied: 1747_fake.png\n",
            "Copied: 2148_fake.png\n",
            "Copied: 2099_fake.png\n",
            "Copied: 1927_fake.png\n",
            "Copied: 2280_fake.png\n",
            "Copied: 2309_fake.png\n",
            "Copied: 2170_fake.png\n",
            "Copied: 2369_fake.png\n",
            "Copied: 1421_fake.png\n",
            "Copied: 2403_fake.png\n",
            "Copied: 1801_fake.png\n",
            "Copied: 1675_fake.png\n",
            "Copied: 1708_fake.png\n",
            "Copied: 1505_fake.png\n",
            "Copied: 2345_fake.png\n",
            "Copied: 2442_fake.png\n",
            "Copied: 1439_fake.png\n",
            "Copied: 1685_fake.png\n",
            "Copied: 1660_fake.png\n",
            "Copied: 2011_fake.png\n",
            "Copied: 2303_fake.png\n",
            "Copied: 2264_fake.png\n",
            "Copied: 2067_fake.png\n",
            "Copied: 1465_fake.png\n",
            "Copied: 1553_fake.png\n",
            "Copied: 2219_fake.png\n",
            "Copied: 2355_fake.png\n",
            "Copied: 2180_fake.png\n",
            "Copied: 2038_fake.png\n",
            "Copied: 1700_fake.png\n",
            "Copied: 2223_fake.png\n",
            "Copied: 1930_fake.png\n",
            "Copied: 2459_fake.png\n",
            "Copied: 2110_fake.png\n",
            "Copied: 1914_fake.png\n",
            "Copied: 1998_fake.png\n",
            "Copied: 1407_fake.png\n",
            "Copied: 2136_fake.png\n",
            "Copied: 2173_fake.png\n",
            "Copied: 2434_fake.png\n",
            "Copied: 1604_fake.png\n",
            "Copied: 1924_fake.png\n",
            "Copied: 1873_fake.png\n",
            "Copied: 1965_fake.png\n",
            "Copied: 2004_fake.png\n",
            "Copied: 1688_fake.png\n",
            "Copied: 1568_fake.png\n",
            "Copied: 2203_fake.png\n",
            "Copied: 2087_fake.png\n",
            "Copied: 2164_fake.png\n",
            "Copied: 2492_fake.png\n",
            "Copied: 2122_fake.png\n",
            "Copied: 2310_fake.png\n",
            "Copied: 1723_fake.png\n",
            "Copied: 1759_fake.png\n",
            "Copied: 2196_fake.png\n",
            "Copied: 2356_fake.png\n",
            "Copied: 1514_fake.png\n",
            "Copied: 1911_fake.png\n",
            "Copied: 1863_fake.png\n",
            "Copied: 2485_fake.png\n",
            "Copied: 1937_fake.png\n",
            "Copied: 2234_fake.png\n",
            "Copied: 1624_fake.png\n",
            "Copied: 1682_fake.png\n",
            "Copied: 1684_fake.png\n",
            "Copied: 2254_fake.png\n",
            "Copied: 2193_fake.png\n",
            "Copied: 2205_fake.png\n",
            "Copied: 1874_fake.png\n",
            "Copied: 1570_fake.png\n",
            "Copied: 1468_fake.png\n",
            "Copied: 1550_fake.png\n",
            "Copied: 1654_fake.png\n",
            "Copied: 1906_fake.png\n",
            "Copied: 1646_fake.png\n",
            "Copied: 2233_fake.png\n",
            "Copied: 1813_fake.png\n",
            "Copied: 2217_fake.png\n",
            "Copied: 1819_fake.png\n",
            "Copied: 2335_fake.png\n",
            "Copied: 2281_fake.png\n",
            "Copied: 2446_fake.png\n",
            "Copied: 1530_fake.png\n",
            "Copied: 2244_fake.png\n",
            "Copied: 2018_fake.png\n",
            "Copied: 2304_fake.png\n",
            "Copied: 1666_fake.png\n",
            "Copied: 2097_fake.png\n",
            "Copied: 1600_fake.png\n",
            "Copied: 1703_fake.png\n",
            "Copied: 1556_fake.png\n",
            "Copied: 1619_fake.png\n",
            "Copied: 1536_fake.png\n",
            "Copied: 2069_fake.png\n",
            "Copied: 2450_fake.png\n",
            "Copied: 1910_fake.png\n",
            "Copied: 2407_fake.png\n",
            "Copied: 2394_fake.png\n",
            "Copied: 1676_fake.png\n",
            "Copied: 1617_fake.png\n",
            "Copied: 1446_fake.png\n",
            "Copied: 1645_fake.png\n",
            "Copied: 2511_fake.png\n",
            "Copied: 2055_fake.png\n",
            "Copied: 1732_fake.png\n",
            "Copied: 2255_fake.png\n",
            "Copied: 2167_fake.png\n",
            "Copied: 1554_fake.png\n",
            "Copied: 1795_fake.png\n",
            "Copied: 2200_fake.png\n",
            "Copied: 2252_fake.png\n",
            "Copied: 2121_fake.png\n",
            "Copied: 1528_fake.png\n",
            "Copied: 2299_fake.png\n",
            "Copied: 2157_fake.png\n",
            "Copied: 2140_fake.png\n",
            "Copied: 1384_fake.png\n",
            "Copied: 1427_fake.png\n",
            "Copied: 2476_fake.png\n",
            "Copied: 2046_fake.png\n",
            "Copied: 1574_fake.png\n",
            "Copied: 1428_fake.png\n",
            "Copied: 1893_fake.png\n",
            "Copied: 1897_fake.png\n",
            "Copied: 1775_fake.png\n",
            "Copied: 2328_fake.png\n",
            "Copied: 2481_fake.png\n",
            "Copied: 1538_fake.png\n",
            "Copied: 1956_fake.png\n",
            "Copied: 2517_fake.png\n",
            "Copied: 2019_fake.png\n",
            "Copied: 2521_fake.png\n",
            "Copied: 1725_fake.png\n",
            "Copied: 1953_fake.png\n",
            "Copied: 2149_fake.png\n",
            "Copied: 1618_fake.png\n",
            "Copied: 1750_fake.png\n",
            "Copied: 2077_fake.png\n",
            "Copied: 2322_fake.png\n",
            "Copied: 1720_fake.png\n",
            "Copied: 2412_fake.png\n",
            "Copied: 1847_fake.png\n",
            "Copied: 2368_fake.png\n",
            "Copied: 2308_fake.png\n",
            "Copied: 1890_fake.png\n",
            "Copied: 2286_fake.png\n",
            "Copied: 1944_fake.png\n",
            "Copied: 2375_fake.png\n",
            "Copied: 1631_fake.png\n",
            "Copied: 1842_fake.png\n",
            "Copied: 2425_fake.png\n",
            "Copied: 2307_fake.png\n",
            "Copied: 1894_fake.png\n",
            "Copied: 2245_fake.png\n",
            "Copied: 1745_fake.png\n",
            "Copied: 1912_fake.png\n",
            "Copied: 1656_fake.png\n",
            "Copied: 2293_fake.png\n",
            "Copied: 1434_fake.png\n",
            "Copied: 1969_fake.png\n",
            "Copied: 2484_fake.png\n",
            "Copied: 1470_fake.png\n",
            "Copied: 1781_fake.png\n",
            "Copied: 2436_fake.png\n",
            "Copied: 1737_fake.png\n",
            "Copied: 1680_fake.png\n",
            "Copied: 2366_fake.png\n",
            "Copied: 2402_fake.png\n",
            "Copied: 2493_fake.png\n",
            "Copied: 2228_fake.png\n",
            "Copied: 2239_fake.png\n",
            "Copied: 1506_fake.png\n",
            "Copied: 1560_fake.png\n",
            "Copied: 1501_fake.png\n",
            "Copied: 1710_fake.png\n",
            "Copied: 2003_fake.png\n",
            "Copied: 2100_fake.png\n",
            "Copied: 1653_fake.png\n",
            "Copied: 1852_fake.png\n",
            "Copied: 2324_fake.png\n",
            "Copied: 1804_fake.png\n",
            "Copied: 2445_fake.png\n",
            "Copied: 2111_fake.png\n",
            "Copied: 1952_fake.png\n",
            "Copied: 1461_fake.png\n",
            "Copied: 2378_fake.png\n",
            "Copied: 1789_fake.png\n",
            "Copied: 2169_fake.png\n",
            "Copied: 1800_fake.png\n",
            "Copied: 1811_fake.png\n",
            "Copied: 1904_fake.png\n",
            "Copied: 1980_fake.png\n",
            "Copied: 1696_fake.png\n",
            "Copied: 2520_fake.png\n",
            "Copied: 1378_fake.png\n",
            "Copied: 1784_fake.png\n",
            "Copied: 2082_fake.png\n",
            "Copied: 2116_fake.png\n",
            "Copied: 2095_fake.png\n",
            "Copied: 2129_fake.png\n",
            "Copied: 2211_fake.png\n",
            "Copied: 2461_fake.png\n",
            "Copied: 2268_fake.png\n",
            "Copied: 2296_fake.png\n",
            "Copied: 2174_fake.png\n",
            "Copied: 1742_fake.png\n",
            "Copied: 1817_fake.png\n",
            "Copied: 1770_fake.png\n",
            "Copied: 2232_fake.png\n",
            "Copied: 2083_fake.png\n",
            "Copied: 2392_fake.png\n",
            "Copied: 1822_fake.png\n",
            "Copied: 2343_fake.png\n",
            "Copied: 2092_fake.png\n",
            "Copied: 2112_fake.png\n",
            "Copied: 1690_fake.png\n",
            "Copied: 2437_fake.png\n",
            "Copied: 1586_fake.png\n",
            "Copied: 1867_fake.png\n",
            "Copied: 1967_fake.png\n",
            "Copied: 2410_fake.png\n",
            "Copied: 2411_fake.png\n",
            "Copied: 1582_fake.png\n",
            "Copied: 1584_fake.png\n",
            "Copied: 1826_fake.png\n",
            "Copied: 2185_fake.png\n",
            "Copied: 2370_fake.png\n",
            "Copied: 2238_fake.png\n",
            "Copied: 2218_fake.png\n",
            "Copied: 2249_fake.png\n",
            "Copied: 2323_fake.png\n",
            "Copied: 2399_fake.png\n",
            "Copied: 2285_fake.png\n",
            "Copied: 1753_fake.png\n",
            "Copied: 1449_fake.png\n",
            "Copied: 2240_fake.png\n",
            "Copied: 1495_fake.png\n",
            "Copied: 2224_fake.png\n",
            "Copied: 1928_fake.png\n",
            "Copied: 1810_fake.png\n",
            "Copied: 2061_fake.png\n",
            "Copied: 2371_fake.png\n",
            "Copied: 1651_fake.png\n",
            "Copied: 2263_fake.png\n",
            "Copied: 2331_fake.png\n",
            "Copied: 1431_fake.png\n",
            "Copied: 1797_fake.png\n",
            "Copied: 2192_fake.png\n",
            "Copied: 2030_fake.png\n",
            "Copied: 1426_fake.png\n",
            "Copied: 1647_fake.png\n",
            "Copied: 1450_fake.png\n",
            "Copied: 1435_fake.png\n",
            "Copied: 1544_fake.png\n",
            "Copied: 1614_fake.png\n",
            "Copied: 1447_fake.png\n",
            "Copied: 2318_fake.png\n",
            "Copied: 2444_fake.png\n",
            "Copied: 1882_fake.png\n",
            "Copied: 1729_fake.png\n",
            "Copied: 2404_fake.png\n",
            "Copied: 2242_fake.png\n",
            "Copied: 1743_fake.png\n",
            "Copied: 2154_fake.png\n",
            "Copied: 1979_fake.png\n",
            "Copied: 1989_fake.png\n",
            "Copied: 2107_fake.png\n",
            "Copied: 2135_fake.png\n",
            "Copied: 1532_fake.png\n",
            "Copied: 1486_fake.png\n",
            "Copied: 2257_fake.png\n",
            "Copied: 1790_fake.png\n",
            "Copied: 1640_fake.png\n",
            "Copied: 1903_fake.png\n",
            "Copied: 1649_fake.png\n",
            "Copied: 1833_fake.png\n",
            "Copied: 2451_fake.png\n",
            "Copied: 2279_fake.png\n",
            "Copied: 2413_fake.png\n",
            "Copied: 1563_fake.png\n",
            "Copied: 1422_fake.png\n",
            "Copied: 1370_fake.png\n",
            "Copied: 2074_fake.png\n",
            "Copied: 1860_fake.png\n",
            "Copied: 2478_fake.png\n",
            "Copied: 2013_fake.png\n",
            "Copied: 2504_fake.png\n",
            "Copied: 1854_fake.png\n",
            "Copied: 2398_fake.png\n",
            "Copied: 1655_fake.png\n",
            "Copied: 1409_fake.png\n",
            "Copied: 2439_fake.png\n",
            "Copied: 1669_fake.png\n",
            "Copied: 1793_fake.png\n",
            "Copied: 2269_fake.png\n",
            "Copied: 1932_fake.png\n",
            "Copied: 1674_fake.png\n",
            "Copied: 1577_fake.png\n",
            "Copied: 1643_fake.png\n",
            "Copied: 2357_fake.png\n",
            "Copied: 2181_fake.png\n",
            "Copied: 2256_fake.png\n",
            "Copied: 2438_fake.png\n",
            "Copied: 2464_fake.png\n",
            "Copied: 1525_fake.png\n",
            "Copied: 1769_fake.png\n",
            "Copied: 1612_fake.png\n",
            "Copied: 1846_fake.png\n",
            "Copied: 1718_fake.png\n",
            "Copied: 2195_fake.png\n",
            "Copied: 1740_fake.png\n",
            "Copied: 1839_fake.png\n",
            "Copied: 1792_fake.png\n",
            "Copied: 2184_fake.png\n",
            "Copied: 1591_fake.png\n",
            "Copied: 2430_fake.png\n",
            "Copied: 1886_fake.png\n",
            "Copied: 2367_fake.png\n",
            "Copied: 1622_fake.png\n",
            "Copied: 1503_fake.png\n",
            "Copied: 1621_fake.png\n",
            "Copied: 2361_fake.png\n",
            "Copied: 2166_fake.png\n",
            "Copied: 2317_fake.png\n",
            "Copied: 1936_fake.png\n",
            "Copied: 2101_fake.png\n",
            "Copied: 2313_fake.png\n",
            "Copied: 2333_fake.png\n",
            "Copied: 2388_fake.png\n",
            "Copied: 2130_fake.png\n",
            "Copied: 1779_fake.png\n",
            "Copied: 2409_fake.png\n",
            "Copied: 1535_fake.png\n",
            "Copied: 2212_fake.png\n",
            "Copied: 2126_fake.png\n",
            "Copied: 2172_fake.png\n",
            "Copied: 2120_fake.png\n",
            "Copied: 2040_fake.png\n",
            "Copied: 2052_fake.png\n",
            "Copied: 1760_fake.png\n",
            "Copied: 2015_fake.png\n",
            "Copied: 1433_fake.png\n",
            "Copied: 1878_fake.png\n",
            "Copied: 1721_fake.png\n",
            "Copied: 1452_fake.png\n",
            "Copied: 1997_fake.png\n",
            "Copied: 2084_fake.png\n",
            "Copied: 1945_fake.png\n",
            "Copied: 2389_fake.png\n",
            "Copied: 1869_fake.png\n",
            "Copied: 2009_fake.png\n",
            "Copied: 2337_fake.png\n",
            "Copied: 1872_fake.png\n",
            "Copied: 1920_fake.png\n",
            "Copied: 2147_fake.png\n",
            "Copied: 2418_fake.png\n",
            "Copied: 1403_fake.png\n",
            "Copied: 1825_fake.png\n",
            "Copied: 2480_fake.png\n",
            "Copied: 1440_fake.png\n",
            "Copied: 2054_fake.png\n",
            "Copied: 2266_fake.png\n",
            "Copied: 2326_fake.png\n",
            "Copied: 1423_fake.png\n",
            "Copied: 2393_fake.png\n",
            "Copied: 1752_fake.png\n",
            "Copied: 1978_fake.png\n",
            "Copied: 1629_fake.png\n",
            "Copied: 2070_fake.png\n",
            "Copied: 1851_fake.png\n",
            "Copied: 1592_fake.png\n",
            "Copied: 2041_fake.png\n",
            "Copied: 1724_fake.png\n",
            "Copied: 1834_fake.png\n",
            "Copied: 1441_fake.png\n",
            "Copied: 2153_fake.png\n",
            "Copied: 2202_fake.png\n",
            "Copied: 1783_fake.png\n",
            "Copied: 2090_fake.png\n",
            "Copied: 1850_fake.png\n",
            "Copied: 1866_fake.png\n",
            "Copied: 1603_fake.png\n",
            "Copied: 1968_fake.png\n",
            "Copied: 2113_fake.png\n",
            "Copied: 2029_fake.png\n",
            "Copied: 1738_fake.png\n",
            "Copied: 1517_fake.png\n",
            "Copied: 1394_fake.png\n",
            "Copied: 1642_fake.png\n",
            "Copied: 2360_fake.png\n",
            "Copied: 1502_fake.png\n",
            "Copied: 1695_fake.png\n",
            "Copied: 1611_fake.png\n",
            "Copied: 2284_fake.png\n",
            "Copied: 2519_fake.png\n",
            "Copied: 2362_fake.png\n",
            "Copied: 1879_fake.png\n",
            "Copied: 1594_fake.png\n",
            "Copied: 1523_fake.png\n",
            "Copied: 1853_fake.png\n",
            "Copied: 2427_fake.png\n",
            "Copied: 2160_fake.png\n",
            "Copied: 2474_fake.png\n",
            "Copied: 1843_fake.png\n",
            "Copied: 1632_fake.png\n",
            "Copied: 1580_fake.png\n",
            "Copied: 2479_fake.png\n",
            "Copied: 1453_fake.png\n",
            "Copied: 1455_fake.png\n",
            "Copied: 1543_fake.png\n",
            "Copied: 2500_fake.png\n",
            "Copied: 1559_fake.png\n",
            "Copied: 2406_fake.png\n",
            "Copied: 2364_fake.png\n",
            "Copied: 1694_fake.png\n",
            "Copied: 2510_fake.png\n",
            "Copied: 1746_fake.png\n",
            "Copied: 2414_fake.png\n",
            "Copied: 1705_fake.png\n",
            "Copied: 1807_fake.png\n",
            "Copied: 1399_fake.png\n",
            "Copied: 1907_fake.png\n",
            "Copied: 2032_fake.png\n",
            "Copied: 2213_fake.png\n",
            "Copied: 1970_fake.png\n",
            "Copied: 1462_fake.png\n",
            "Copied: 2215_fake.png\n",
            "Copied: 2131_fake.png\n",
            "Copied: 2225_fake.png\n",
            "Copied: 2339_fake.png\n",
            "Copied: 2106_fake.png\n",
            "Copied: 2071_fake.png\n",
            "Copied: 2080_fake.png\n",
            "Copied: 1636_fake.png\n",
            "Copied: 2128_fake.png\n",
            "Copied: 1527_fake.png\n",
            "Copied: 2259_fake.png\n",
            "Copied: 2091_fake.png\n",
            "Copied: 1547_fake.png\n",
            "Copied: 2420_fake.png\n",
            "Copied: 1870_fake.png\n",
            "Copied: 1856_fake.png\n",
            "Copied: 1484_fake.png\n",
            "Copied: 1861_fake.png\n",
            "Copied: 1375_fake.png\n",
            "Copied: 2021_fake.png\n",
            "Copied: 1716_fake.png\n",
            "Copied: 2489_fake.png\n",
            "Copied: 1815_fake.png\n",
            "Copied: 2487_fake.png\n",
            "Copied: 1942_fake.png\n",
            "Copied: 2163_fake.png\n",
            "Copied: 1689_fake.png\n",
            "Copied: 1791_fake.png\n",
            "Copied: 1925_fake.png\n",
            "Copied: 1458_fake.png\n",
            "Copied: 1412_fake.png\n",
            "Copied: 1673_fake.png\n",
            "Copied: 2188_fake.png\n",
            "Copied: 2274_fake.png\n",
            "Copied: 2031_fake.png\n",
            "Copied: 1583_fake.png\n",
            "Copied: 1442_fake.png\n",
            "Copied: 1494_fake.png\n",
            "Copied: 1386_fake.png\n",
            "Copied: 1809_fake.png\n",
            "Copied: 2390_fake.png\n",
            "Copied: 1962_fake.png\n",
            "Copied: 2262_fake.png\n",
            "Copied: 2096_fake.png\n",
            "Copied: 2158_fake.png\n",
            "Copied: 1777_fake.png\n",
            "Copied: 2104_fake.png\n",
            "Copied: 1875_fake.png\n",
            "Copied: 2168_fake.png\n",
            "Copied: 2206_fake.png\n",
            "Copied: 1635_fake.png\n",
            "Copied: 2207_fake.png\n",
            "Copied: 1481_fake.png\n",
            "Copied: 1581_fake.png\n",
            "Copied: 1460_fake.png\n",
            "Copied: 2197_fake.png\n",
            "Copied: 1396_fake.png\n",
            "Copied: 1845_fake.png\n",
            "Copied: 1916_fake.png\n",
            "Copied: 1634_fake.png\n",
            "Copied: 1736_fake.png\n",
            "Copied: 1900_fake.png\n",
            "Copied: 2007_fake.png\n",
            "Copied: 1913_fake.png\n",
            "Copied: 2150_fake.png\n",
            "Copied: 1595_fake.png\n",
            "Copied: 1587_fake.png\n",
            "Copied: 2400_fake.png\n",
            "Copied: 1551_fake.png\n",
            "Copied: 2352_fake.png\n",
            "Copied: 2000_fake.png\n",
            "Copied: 1662_fake.png\n",
            "Copied: 1693_fake.png\n",
            "Copied: 1405_fake.png\n",
            "Copied: 2105_fake.png\n",
            "Copied: 2358_fake.png\n",
            "Copied: 1884_fake.png\n",
            "Copied: 2507_fake.png\n",
            "Copied: 1558_fake.png\n",
            "Copied: 2363_fake.png\n",
            "Copied: 2049_fake.png\n",
            "Copied: 2230_fake.png\n",
            "Copied: 1841_fake.png\n",
            "Copied: 1728_fake.png\n",
            "Copied: 2422_fake.png\n",
            "Copied: 2469_fake.png\n",
            "Copied: 1722_fake.png\n",
            "Copied: 1971_fake.png\n",
            "Copied: 1579_fake.png\n",
            "Copied: 1572_fake.png\n",
            "Copied: 2283_fake.png\n",
            "Copied: 2453_fake.png\n",
            "Copied: 2171_fake.png\n",
            "Copied: 2222_fake.png\n",
            "Copied: 2327_fake.png\n",
            "Copied: 2127_fake.png\n",
            "Copied: 1991_fake.png\n",
            "Copied: 2020_fake.png\n",
            "Copied: 1838_fake.png\n",
            "Copied: 1473_fake.png\n",
            "Copied: 2078_fake.png\n",
            "Copied: 1889_fake.png\n",
            "Copied: 1963_fake.png\n",
            "Copied: 2190_fake.png\n",
            "Copied: 2183_fake.png\n",
            "Copied: 2016_fake.png\n",
            "Copied: 2341_fake.png\n",
            "Copied: 2432_fake.png\n",
            "Copied: 2297_fake.png\n",
            "Copied: 2161_fake.png\n",
            "Copied: 1504_fake.png\n",
            "Copied: 2460_fake.png\n",
            "Copied: 2518_fake.png\n",
            "Copied: 2039_fake.png\n",
            "Copied: 1699_fake.png\n",
            "Copied: 1697_fake.png\n",
            "Copied: 2051_fake.png\n",
            "Copied: 1891_fake.png\n",
            "Copied: 2117_fake.png\n",
            "Copied: 2226_fake.png\n",
            "Copied: 2506_fake.png\n",
            "Copied: 2301_fake.png\n",
            "Copied: 1917_fake.png\n",
            "Copied: 2179_fake.png\n",
            "Copied: 2187_fake.png\n",
            "Copied: 2014_fake.png\n",
            "Copied: 1416_fake.png\n",
            "Copied: 1529_fake.png\n",
            "Copied: 1498_fake.png\n",
            "Copied: 2515_fake.png\n",
            "Copied: 1692_fake.png\n",
            "Copied: 1726_fake.png\n",
            "Copied: 1607_fake.png\n",
            "Copied: 2490_fake.png\n",
            "Copied: 2145_fake.png\n",
            "Copied: 1955_fake.png\n",
            "Copied: 2380_fake.png\n",
            "Copied: 2395_fake.png\n",
            "Copied: 2288_fake.png\n",
            "Copied: 1778_fake.png\n",
            "Copied: 2329_fake.png\n",
            "Copied: 2501_fake.png\n",
            "Copied: 1408_fake.png\n",
            "Copied: 1756_fake.png\n",
            "Copied: 1661_fake.png\n",
            "Copied: 2123_fake.png\n",
            "Copied: 2133_fake.png\n",
            "Copied: 1919_fake.png\n",
            "Copied: 1881_fake.png\n",
            "Copied: 2053_fake.png\n",
            "Copied: 2290_fake.png\n",
            "Copied: 2282_fake.png\n",
            "Copied: 1534_fake.png\n",
            "Copied: 2509_fake.png\n",
            "Copied: 1385_fake.png\n",
            "Copied: 2178_fake.png\n",
            "Copied: 2017_fake.png\n",
            "Copied: 1859_fake.png\n",
            "Copied: 2496_fake.png\n",
            "Copied: 1821_fake.png\n",
            "Copied: 2102_fake.png\n",
            "Copied: 2373_fake.png\n",
            "Copied: 1896_fake.png\n",
            "Copied: 2494_fake.png\n",
            "Copied: 2514_fake.png\n",
            "Copied: 1938_fake.png\n",
            "Copied: 1812_fake.png\n",
            "Copied: 2433_fake.png\n",
            "Copied: 2151_fake.png\n",
            "Copied: 2251_fake.png\n",
            "Copied: 1739_fake.png\n",
            "Copied: 1829_fake.png\n",
            "Copied: 2010_fake.png\n",
            "Copied: 2235_fake.png\n",
            "Copied: 1714_fake.png\n",
            "Copied: 1383_fake.png\n",
            "Copied: 1496_fake.png\n",
            "Copied: 2449_fake.png\n",
            "Copied: 1948_fake.png\n",
            "Copied: 1456_fake.png\n",
            "Copied: 1469_fake.png\n",
            "Copied: 1389_fake.png\n",
            "Copied: 2441_fake.png\n",
            "Copied: 2146_fake.png\n",
            "Copied: 1975_fake.png\n",
            "Copied: 1602_fake.png\n",
            "Copied: 1712_fake.png\n",
            "Copied: 1780_fake.png\n",
            "Copied: 1671_fake.png\n",
            "Copied: 1406_fake.png\n",
            "Copied: 1476_fake.png\n",
            "Copied: 1648_fake.png\n",
            "Copied: 1691_fake.png\n",
            "Copied: 2194_fake.png\n",
            "Copied: 1926_fake.png\n",
            "Copied: 1555_fake.png\n",
            "Copied: 2022_fake.png\n",
            "Copied: 1735_fake.png\n",
            "Copied: 1983_fake.png\n",
            "Copied: 2248_fake.png\n",
            "Copied: 1877_fake.png\n",
            "Copied: 1717_fake.png\n",
            "Copied: 2381_fake.png\n",
            "Copied: 2085_fake.png\n",
            "Copied: 1816_fake.png\n",
            "Copied: 1849_fake.png\n",
            "Copied: 2294_fake.png\n",
            "Copied: 2216_fake.png\n",
            "Copied: 1573_fake.png\n",
            "Copied: 1885_fake.png\n",
            "Copied: 1610_fake.png\n",
            "Copied: 1491_fake.png\n",
            "Copied: 1552_fake.png\n",
            "Copied: 1395_fake.png\n",
            "Copied: 1764_fake.png\n",
            "Copied: 1883_fake.png\n",
            "Copied: 1844_fake.png\n",
            "Copied: 2134_fake.png\n",
            "Copied: 1429_fake.png\n",
            "Copied: 2114_fake.png\n",
            "Copied: 2125_fake.png\n",
            "Copied: 1933_fake.png\n",
            "Copied: 2321_fake.png\n",
            "Copied: 2198_fake.png\n",
            "Copied: 1593_fake.png\n",
            "Copied: 1454_fake.png\n",
            "Copied: 1644_fake.png\n",
            "Copied: 1548_fake.png\n",
            "Copied: 1401_fake.png\n",
            "Copied: 1934_fake.png\n",
            "Copied: 2456_fake.png\n",
            "Copied: 2220_fake.png\n",
            "Copied: 2502_fake.png\n",
            "Copied: 2429_fake.png\n",
            "Copied: 2023_fake.png\n",
            "Copied: 1776_fake.png\n",
            "Copied: 1373_fake.png\n",
            "Copied: 2143_fake.png\n",
            "Copied: 1432_fake.png\n",
            "Copied: 1994_fake.png\n",
            "Copied: 1827_fake.png\n",
            "Copied: 2372_fake.png\n",
            "Copied: 2348_fake.png\n",
            "Copied: 2155_fake.png\n",
            "Copied: 2408_fake.png\n",
            "Copied: 1509_fake.png\n",
            "Copied: 2298_fake.png\n",
            "Copied: 1424_fake.png\n",
            "Copied: 2270_fake.png\n",
            "Copied: 1992_fake.png\n",
            "Copied: 2330_fake.png\n",
            "Copied: 2231_fake.png\n",
            "Copied: 1901_fake.png\n",
            "Copied: 1601_fake.png\n",
            "Copied: 2012_fake.png\n",
            "Copied: 2472_fake.png\n",
            "Copied: 1999_fake.png\n",
            "Copied: 1641_fake.png\n",
            "Copied: 2118_fake.png\n",
            "Copied: 2468_fake.png\n",
            "Copied: 2443_fake.png\n",
            "Copied: 1474_fake.png\n",
            "Copied: 2072_fake.png\n",
            "Copied: 2463_fake.png\n",
            "Copied: 2384_fake.png\n",
            "Copied: 1922_fake.png\n",
            "Copied: 2276_fake.png\n",
            "Copied: 1686_fake.png\n",
            "Copied: 1398_fake.png\n",
            "Copied: 1905_fake.png\n",
            "Copied: 1986_fake.png\n",
            "Copied: 1818_fake.png\n",
            "Copied: 1626_fake.png\n",
            "Copied: 2482_fake.png\n",
            "Copied: 2471_fake.png\n",
            "Copied: 1512_fake.png\n",
            "Copied: 1608_fake.png\n",
            "Copied: 1444_fake.png\n",
            "Copied: 1899_fake.png\n",
            "Copied: 1730_fake.png\n",
            "Copied: 1519_fake.png\n",
            "Copied: 2497_fake.png\n",
            "Copied: 1765_fake.png\n",
            "Copied: 1981_fake.png\n",
            "Copied: 2066_fake.png\n",
            "Copied: 1522_fake.png\n",
            "Copied: 2428_fake.png\n",
            "Copied: 1805_fake.png\n",
            "Copied: 2191_fake.png\n",
            "Copied: 1959_fake.png\n",
            "Copied: 2289_fake.png\n",
            "Copied: 2273_fake.png\n",
            "Copied: 1864_fake.png\n",
            "Copied: 2365_fake.png\n",
            "Copied: 2247_fake.png\n",
            "Copied: 1887_fake.png\n",
            "Copied: 1448_fake.png\n",
            "Copied: 1950_fake.png\n",
            "Copied: 2159_fake.png\n",
            "Copied: 1436_fake.png\n",
            "Copied: 2065_fake.png\n",
            "Copied: 1497_fake.png\n",
            "Copied: 1479_fake.png\n",
            "Copied: 1391_fake.png\n",
            "Copied: 1492_fake.png\n",
            "Copied: 2342_fake.png\n",
            "Copied: 1687_fake.png\n",
            "Copied: 2300_fake.png\n",
            "Copied: 1915_fake.png\n",
            "Copied: 2176_fake.png\n",
            "Copied: 1542_fake.png\n",
            "Copied: 2311_fake.png\n",
            "Copied: 2305_fake.png\n",
            "Copied: 1837_fake.png\n",
            "Copied: 2103_fake.png\n",
            "Copied: 1798_fake.png\n",
            "Copied: 2028_fake.png\n",
            "Copied: 2044_fake.png\n",
            "Copied: 2426_fake.png\n",
            "Copied: 1510_fake.png\n",
            "Copied: 2144_fake.png\n",
            "Copied: 1814_fake.png\n",
            "Copied: 1848_fake.png\n",
            "Copied: 1521_fake.png\n",
            "Copied: 1567_fake.png\n",
            "Copied: 2417_fake.png\n",
            "Copied: 1374_fake.png\n",
            "Copied: 1719_fake.png\n",
            "Copied: 1731_fake.png\n",
            "Copied: 2260_fake.png\n",
            "Copied: 1606_fake.png\n",
            "Copied: 1794_fake.png\n",
            "Copied: 2008_fake.png\n",
            "Copied: 2221_fake.png\n",
            "Copied: 2457_fake.png\n",
            "Copied: 1982_fake.png\n",
            "Copied: 2334_fake.png\n",
            "Copied: 1658_fake.png\n",
            "Copied: 2156_fake.png\n",
            "Copied: 2165_fake.png\n",
            "Copied: 1599_fake.png\n",
            "Copied: 2043_fake.png\n",
            "Copied: 2278_fake.png\n",
            "Copied: 2424_fake.png\n",
            "Copied: 1515_fake.png\n",
            "Copied: 2060_fake.png\n",
            "Copied: 1727_fake.png\n",
            "Copied: 1380_fake.png\n",
            "Copied: 1767_fake.png\n",
            "Copied: 1483_fake.png\n",
            "Copied: 1985_fake.png\n",
            "Copied: 1895_fake.png\n",
            "Copied: 2064_fake.png\n",
            "Copied: 1620_fake.png\n",
            "Copied: 2237_fake.png\n",
            "Copied: 1935_fake.png\n",
            "Copied: 1507_fake.png\n",
            "Copied: 2265_fake.png\n",
            "Copied: 2462_fake.png\n",
            "Copied: 1761_fake.png\n",
            "Copied: 1947_fake.png\n",
            "Copied: 1633_fake.png\n",
            "Copied: 1537_fake.png\n",
            "Copied: 1630_fake.png\n",
            "Copied: 1902_fake.png\n",
            "Copied: 2047_fake.png\n",
            "Copied: 2295_fake.png\n",
            "Copied: 1749_fake.png\n",
            "Copied: 2201_fake.png\n",
            "Copied: 1988_fake.png\n",
            "Copied: 1627_fake.png\n",
            "Copied: 2505_fake.png\n",
            "Copied: 2452_fake.png\n",
            "Copied: 1713_fake.png\n",
            "Copied: 2383_fake.png\n",
            "Copied: 1499_fake.png\n",
            "Copied: 1480_fake.png\n",
            "Copied: 2108_fake.png\n",
            "Copied: 1909_fake.png\n",
            "Copied: 2401_fake.png\n",
            "Copied: 1488_fake.png\n",
            "Copied: 2076_fake.png\n",
            "Copied: 1954_fake.png\n",
            "Copied: 1623_fake.png\n",
            "Copied: 1397_fake.png\n",
            "Copied: 1664_fake.png\n",
            "Copied: 1615_fake.png\n",
            "Copied: 1368_fake.png\n",
            "Copied: 1880_fake.png\n",
            "Copied: 2349_fake.png\n",
            "Copied: 2513_fake.png\n",
            "Copied: 1701_fake.png\n",
            "Copied: 1823_fake.png\n",
            "Copied: 1466_fake.png\n",
            "Copied: 1562_fake.png\n",
            "Copied: 2005_fake.png\n",
            "Copied: 1597_fake.png\n",
            "Copied: 1388_fake.png\n",
            "Copied: 1404_fake.png\n",
            "Copied: 2081_fake.png\n",
            "Copied: 1575_fake.png\n",
            "Copied: 1598_fake.png\n",
            "Copied: 2503_fake.png\n",
            "Copied: 2458_fake.png\n",
            "Copied: 1369_fake.png\n",
            "Copied: 2495_fake.png\n",
            "Copied: 1774_fake.png\n",
            "Copied: 2048_fake.png\n",
            "Copied: 2209_fake.png\n",
            "Copied: 2089_fake.png\n",
            "Copied: 1751_fake.png\n",
            "Copied: 1672_fake.png\n",
            "Copied: 1376_fake.png\n",
            "Copied: 1768_fake.png\n",
            "Copied: 2042_fake.png\n",
            "Copied: 1464_fake.png\n",
            "Copied: 1585_fake.png\n",
            "Copied: 1733_fake.png\n",
            "Copied: 1835_fake.png\n",
            "Copied: 2488_fake.png\n",
            "Copied: 1557_fake.png\n",
            "Copied: 1803_fake.png\n",
            "Copied: 2152_fake.png\n",
            "Copied: 1923_fake.png\n",
            "Copied: 1977_fake.png\n",
            "Copied: 1520_fake.png\n",
            "Copied: 1678_fake.png\n",
            "Copied: 2204_fake.png\n",
            "Copied: 1549_fake.png\n",
            "Copied: 1516_fake.png\n",
            "Copied: 2214_fake.png\n",
            "Copied: 2316_fake.png\n",
            "Copied: 2243_fake.png\n",
            "Copied: 2353_fake.png\n",
            "Copied: 2325_fake.png\n",
            "Copied: 2024_fake.png\n",
            "Copied: 1659_fake.png\n",
            "Copied: 1824_fake.png\n",
            "Copied: 2340_fake.png\n",
            "Copied: 1546_fake.png\n",
            "Copied: 1545_fake.png\n",
            "Copied: 1987_fake.png\n",
            "Copied: 2267_fake.png\n",
            "Copied: 2068_fake.png\n",
            "Copied: 2199_fake.png\n",
            "Copied: 1576_fake.png\n",
            "Copied: 1596_fake.png\n",
            "Copied: 1857_fake.png\n",
            "Copied: 2466_fake.png\n",
            "Copied: 1921_fake.png\n",
            "Copied: 1569_fake.png\n",
            "Copied: 1679_fake.png\n",
            "Copied: 1652_fake.png\n",
            "Copied: 1796_fake.png\n",
            "Copied: 1381_fake.png\n",
            "Copied: 1613_fake.png\n",
            "Copied: 1754_fake.png\n",
            "Copied: 2350_fake.png\n",
            "Copied: 1748_fake.png\n",
            "Copied: 2312_fake.png\n",
            "Copied: 2467_fake.png\n",
            "Copied: 1485_fake.png\n",
            "Copied: 1410_fake.png\n",
            "Copied: 1782_fake.png\n",
            "Copied: 1957_fake.png\n",
            "Copied: 2512_fake.png\n",
            "Copied: 2440_fake.png\n",
            "Copied: 2473_fake.png\n",
            "Copied: 2208_fake.png\n",
            "Copied: 2423_fake.png\n",
            "Copied: 2189_fake.png\n",
            "Copied: 1670_fake.png\n",
            "Copied: 1683_fake.png\n",
            "Copied: 1459_fake.png\n",
            "Copied: 1471_fake.png\n",
            "Copied: 1518_fake.png\n",
            "Copied: 2379_fake.png\n",
            "Copied: 1566_fake.png\n",
            "Copied: 2397_fake.png\n",
            "Copied: 2045_fake.png\n",
            "Copied: 2250_fake.png\n",
            "Copied: 1888_fake.png\n",
            "Copied: 1766_fake.png\n",
            "Copied: 1755_fake.png\n",
            "Copied: 1855_fake.png\n",
            "Copied: 2075_fake.png\n",
            "Copied: 1438_fake.png\n",
            "Copied: 1757_fake.png\n",
            "Copied: 1898_fake.png\n",
            "Copied: 2034_fake.png\n",
            "Copied: 2306_fake.png\n",
            "Copied: 2132_fake.png\n",
            "Copied: 1702_fake.png\n",
            "Copied: 2415_fake.png\n",
            "Copied: 1588_fake.png\n",
            "Copied: 2062_fake.png\n",
            "Copied: 2386_fake.png\n",
            "Copied: 2177_fake.png\n",
            "Copied: 1832_fake.png\n",
            "Copied: 1677_fake.png\n",
            "Copied: 2275_fake.png\n",
            "Copied: 1941_fake.png\n",
            "Copied: 2035_fake.png\n",
            "Copied: 1590_fake.png\n",
            "Copied: 1771_fake.png\n",
            "Copied: 2470_fake.png\n",
            "Copied: 1511_fake.png\n",
            "Copied: 1411_fake.png\n",
            "Copied: 1366_fake.png\n",
            "Copied: 1541_fake.png\n",
            "Copied: 1943_fake.png\n",
            "Copied: 1984_fake.png\n",
            "Copied: 1413_fake.png\n",
            "Copied: 2175_fake.png\n",
            "Copied: 2094_fake.png\n",
            "Copied: 2182_fake.png\n",
            "Copied: 1475_fake.png\n",
            "Copied: 2391_fake.png\n",
            "Copied: 1451_fake.png\n",
            "Copied: 1734_fake.png\n",
            "Copied: 2137_fake.png\n",
            "Copied: 1667_fake.png\n",
            "Copied: 1417_fake.png\n",
            "Copied: 1762_fake.png\n",
            "Copied: 2277_fake.png\n",
            "Copied: 1419_fake.png\n",
            "Copied: 1946_fake.png\n",
            "Copied: 2073_fake.png\n",
            "Copied: 1418_fake.png\n",
            "Copied: 1772_fake.png\n",
            "Copied: 2033_fake.png\n",
            "Copied: 2027_fake.png\n",
            "Copied: 2455_fake.png\n",
            "Copied: 1993_fake.png\n",
            "Copied: 2498_fake.png\n",
            "Copied: 1964_fake.png\n",
            "Copied: 2483_fake.png\n",
            "Copied: 1974_fake.png\n",
            "Copied: 2454_fake.png\n",
            "Copied: 1605_fake.png\n",
            "Copied: 1489_fake.png\n",
            "Copied: 1966_fake.png\n",
            "Copied: 1638_fake.png\n",
            "Copied: 1858_fake.png\n",
            "Copied: 1526_fake.png\n",
            "Copied: 2385_fake.png\n",
            "Copied: 2186_fake.png\n",
            "Copied: 1996_fake.png\n",
            "Copied: 1788_fake.png\n",
            "Copied: 1665_fake.png\n",
            "Copied: 1457_fake.png\n",
            "Copied: 1681_fake.png\n",
            "Copied: 1908_fake.png\n"
          ]
        }
      ],
      "source": [
        "\n",
        "### SAVE ONLY FAKE IMAGES ON DRIVE\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "source_directory = '/content/pytorch-CycleGAN-and-pix2pix/results/prova/test_latest/images'  # Replace with your actual directory path\n",
        "destination_directory = '/content/drive/My Drive/cycleGAN_ony_fake'  # Path in Google Drive\n",
        "\n",
        "\n",
        "import shutil\n",
        "\n",
        "\n",
        "# Step 3: Create destination directory if it doesn't exist\n",
        "if not os.path.exists(destination_directory):\n",
        "    os.makedirs(destination_directory)\n",
        "\n",
        "for filename in os.listdir(source_directory):\n",
        "    if 'fake' in filename:  # Check if 'fake' is in the filename\n",
        "        source_file = os.path.join(source_directory, filename)\n",
        "        destination_file = os.path.join(destination_directory, filename)\n",
        "        shutil.copy(source_file, destination_file)  # Copy the file\n",
        "        print(f\"Copied: {filename}\")  # Optional: print which files are copied\n",
        "\n",
        "\n",
        "# List all files in the directory\n",
        "files = os.listdir(destination_directory)\n",
        "\n",
        "# Count only files (excluding subdirectories)\n",
        "file_count = len([file for file in files if os.path.isfile(os.path.join(destination_directory, file))])\n",
        "\n",
        "print(f'Total number of files in the directory: {file_count}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Link to obtain an example of directory of fake_images for the loveDA urban -> rural transformation\n",
        "### https://drive.google.com/drive/folders/1cRtkWyXsnMFAJA31UNtCSCgN42WEhMz1?usp=drive_link"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Mn5ON_2pgHSC"
      },
      "outputs": [],
      "source": [
        "### TO ELIMINATE MASKS FROM DIRECTORY\n",
        "### ATTENTION BE SURE THAT MASKS ARE NOT IN THE SAME DIRECTORY OF trainA and trainB\n",
        "\n",
        "#folder_path = '/content/Train/Train/Rural/masks_png'  # Replace 'folder_name' with your folder's name\n",
        "#!rm -rf {folder_path}\n",
        "#folder_path = '/content/Train/Train/Urban/masks_png'  # Replace 'folder_name' with your folder's name\n",
        "#!rm -rf {folder_path}\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
